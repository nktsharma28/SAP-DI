{
	"properties": {},
	"description": "C_SalesDocItmPrcgElmntDEX_1GEN1",
	"processes": {
		"snowflake1": {
			"component": "Snowflake",
			"metadata": {
				"label": "Snowflake",
				"x": 333.99999713897705,
				"y": 40,
				"height": 80,
				"width": 120,
				"extensible": true,
				"filesRequired": [
					"script.py"
				],
				"generation": 1,
				"config": {
					"Snowflake_Table_Name": "RAW_S4H_APAC_C_SALESDOCITMPRCGELMNTDEX_1",
					"script": "#---------- Project - ANZ GOLDILOCKS----------#\n# Source System - S4 Hana\n# Target Systen - Snowflake Database\n# Prequisite - Install snowflake-connector-python[pandas]\n# Python Code Version = 0.1\n# Developer - Ankit Sharma\n# Modifiy Date - 02-Jun-2022\n# Enviornment - SAP DI Sandbox\n#---------------------------------------------#\n\nimport pandas as pd\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\nfrom io import StringIO\nimport csv\nimport json\nfrom numpy import nan\napi.send('output','connection started')\nCNT='0'    \nimport time\nimport os\nfrom datetime import datetime\nsnowflake.connector.paramstyle='qmark'\n\nif  api.config.Truncate =='Yes':\n    conn_sf = snowflake.connector.connect(\n    account = api.config.Account,\n    user = api.config.Username,\n    warehouse = api.config.Warehouse,\n    password = api.config.Password,\n    database = api.config.Database,\n    schema = api.config.Schema)\n    table=[api.config.Snowflake_Table_Name]\n    conn_sf.cursor().execute(\"delete from IDENTIFIER(?)\",table)\nelse:\n    conn_sf = snowflake.connector.connect(\n    account =  api.config.Account,\n    user = api.config.Username,\n    warehouse = api.config.Warehouse,\n    password = api.config.Password,\n    database = api.config.Database,\n    schema = api.config.Schema)\n    \ndef on_input(inData):\n    api.send('output','Parsing started blcok1')\n    f= open(\"/vrep/vflow/trigger_C_SALESDOCITMPRCGELMNTDEX_1.txt\",\"w\")\n    f.write(\"File Genrated\")\n\n    # read body\n    data = StringIO(inData.body)\n    on_input.has_been_called = True\n\n    # read attributes\n    var = json.dumps(inData.attributes)\n    result = json.loads(var)\n    \n    api.send('output',result)\n    # from here we start json parsing\n    \n    if result['message.lastBatch']==False:\n        \n        ABAP = result['ABAP']\n        Fields = ABAP['Fields']\n    \n        # creating an empty list for headers\n        columns = []\n        for item in Fields:\n            columns.append(item['Name'])  \n    \n        # data mapping using headers & saving into pandas dataframe\n        df = pd.read_csv(data, index_col = False, names = columns,dtype=str)\n\n        df=df.replace(['undefined'],nan)\n        df=df.replace(['9999-99-99'],nan)\n        df=df.replace(['9999-99-99T99:99:99.9999999'],nan)\n        \"\"\"\n        df['CREATIONDATE']=df['CREATIONDATE'].str[:4]+'-'+ df['CREATIONDATE'].str.slice(+4,-2) +'-'+df['CREATIONDATE'].str.slice(+6,+8)\n        df['LASTCHANGEDATE']=df['LASTCHANGEDATE'].str[:4]+'-'+ df['LASTCHANGEDATE'].str.slice(+4,-2) +'-'+df['LASTCHANGEDATE'].str.slice(+6,+8)\n        df['CROSSPLANTSTATUSVALIDITYDATE']=df['CROSSPLANTSTATUSVALIDITYDATE'].str[:4]+'-'+ df['CROSSPLANTSTATUSVALIDITYDATE'].str.slice(+4,-2) +'-'+df['CROSSPLANTSTATUSVALIDITYDATE'].str.slice(+6,+8)\n        df['SALESSTATUSVALIDITYDATE']=df['SALESSTATUSVALIDITYDATE'].str[:4]+'-'+ df['SALESSTATUSVALIDITYDATE'].str.slice(+4,-2) +'-'+df['SALESSTATUSVALIDITYDATE'].str.slice(+6,+8)\n        df['VALIDITYSTARTDATE']=df['VALIDITYSTARTDATE'].str[:4]+'-'+ df['VALIDITYSTARTDATE'].str.slice(+4,-2) +'-'+df['VALIDITYSTARTDATE'].str.slice(+6,+8)\n        df['PRODUCTVALIDSTARTDATE']=df['PRODUCTVALIDSTARTDATE'].str[:4]+'-'+ df['PRODUCTVALIDSTARTDATE'].str.slice(+4,-2) +'-'+df['PRODUCTVALIDSTARTDATE'].str.slice(+6,+8)\n        df['PRODUCTVALIDENDDATE']=df['PRODUCTVALIDENDDATE'].str[:4]+'-'+ df['PRODUCTVALIDENDDATE'].str.slice(+4,-2) +'-'+df['PRODUCTVALIDENDDATE'].str.slice(+6,+8)\n        \"\"\"\n        df=df.replace(['0000-00-00'],nan)\n        \n        df.rename(columns = {'/1DH/CDS_VIEW':'CDS_VIEW','/1DH/OPERATION':'OPERATION'}, inplace = True)\n        df.insert(0, 'LAST_UPDATE_DATE', pd.to_datetime('now').replace(microsecond=0))\n        df['LAST_UPDATE_DATE'] = df['LAST_UPDATE_DATE'].dt.tz_localize('UTC')\n        temp_cols=df.columns.tolist()\n        new_cols=temp_cols[1:] + temp_cols[0:1]\n      \n        df=df[new_cols]\n\n        # here you can prepare your data,\n        # e.g. columns selection or records filtering\n    \n        df_csv = df.to_csv(index = False, header = True)\n        api.send('output',df_csv)\n        table=api.config.Snowflake_Table_Name\n        \n        #Loading data into snowflake table\n        api.send('output','Load started')\n        #df.to_csv(r'/vrep/vflow/Error_I_Address.csv', index=False)\n        #To load data into Snowflake DB using write_pandas function\n    \n        \n\n        \n        \n        try:\n                    \n            CNT='1'\n            \n            inserted_rows = write_pandas(conn = conn_sf, df = df,table_name = table,compression=\"snappy\",parallel=90, quote_identifiers = False)\n            api.send('output',str(inserted_rows))\n            api.send('output',\"Load Completed\")\n            os.remove(\"/vrep/vflow/trigger_C_SALESDOCITMPRCGELMNTDEX_1.txt\")\n            api.send('output','file deleted')\n            api.send('output2',CNT)\n\n        except Exception as e:\n            \n            api.send('output',str('Error while loading data'))\n            date = datetime.now().strftime(\"%Y_%m_%d-%I:%M:%S\")\n            df.to_csv(r'/vrep/vflow/Error_RAW_S4H_C_SALESDOCITMPRCGELMNTDEX_1.csv_{}'.format(date), index=False)\n            api.send('output',str('Failed data pushed to error file in path /vrep/vflow/'))\n            code = 1\n            text = str(e)\n            details = api.Table([['key', 'value']])\n            api.send('output',str(e))\n            raise e   \n          \n        \n        \n    else:\n        api.send('output','No incoming data')\n        api.send('stop','Stop graph')\n\ndef on_input2(inData2):\n    data2 = StringIO(inData2)\n\n    api.send('output','Parsing started blck2')\n    api.send('stop',str(data2))\n    \n\napi.set_port_callback('input1', on_input)\napi.set_port_callback('input2', on_input2)\n        \n\n\n",
					"Account": "Kraft.east-us-2.privatelink",
					"Username": "khc_sapdi_conn_user_dev",
					"Password": "9xXC=lS_3/_^4}v6",
					"Database": "DEV_KHC_INGEST",
					"Schema": "ORDER_MANAGEMENT",
					"Warehouse": "dev_cloud_analytics_platform",
					"Role": "dev_khc_sapdi",
					"Truncate": "Yes"
				},
				"additionalinports": [
					{
						"name": "input2",
						"type": "string"
					}
				],
				"additionaloutports": [
					{
						"name": "stop",
						"type": "message"
					},
					{
						"name": "output2",
						"type": "string"
					}
				]
			}
		},
		"graphterminator1": {
			"component": "com.sap.util.graphTerminator",
			"metadata": {
				"label": "Graph Terminator",
				"x": 626.9999952316284,
				"y": 165.99999976158142,
				"height": 80,
				"width": 120,
				"generation": 1,
				"config": {}
			}
		},
		"sapabapoperator1": {
			"component": "com.sap.abap.sap",
			"metadata": {
				"label": "ABAP CDS Reader V2",
				"x": 72.99999904632568,
				"y": 32,
				"height": 80,
				"width": 120,
				"extensible": true,
				"generation": 1,
				"config": {
					"connectionID": "S4H_ANZ",
					"operatorID": "com.sap.abap.cds.reader.v2",
					"subscriptionType": "New",
					"action": "Replication",
					"wireformat": "Required Conversions",
					"cdsname": "C_SalesDocItmPrcgElmntDEX_1",
					"subscriptionName": "C_SalesDocItmPrcgElmntDEX_1_0109_3"
				},
				"additionaloutports": [
					{
						"name": "outMessageData",
						"type": "message"
					}
				]
			}
		},
		"wiretap1": {
			"component": "com.sap.util.wiretap",
			"metadata": {
				"label": "Wiretap",
				"x": 626.9999952316284,
				"y": 45.99999976158142,
				"height": 80,
				"width": 120,
				"generation": 1,
				"ui": "dynpath",
				"config": {}
			}
		},
		"python3operator1": {
			"component": "com.sap.system.python3Operator",
			"metadata": {
				"label": "Python3 Operator",
				"x": 72.99999904632568,
				"y": 152,
				"height": 80,
				"width": 120,
				"extensible": true,
				"filesRequired": [
					"script.py"
				],
				"generation": 1,
				"config": {
					"script": "from os.path import exists\r\n\r\n\r\nimport time\r\ntime.sleep(30)\r\n\r\nfile_exists = exists(\"/vrep/vflow/trigger_C_SALESDOCITMPRCGELMNTDEX_1.txt\")\r\nif file_exists:\r\n    api.send('output2','file_exists')\r\nelse:\r\n            \r\n    api.send('output2','file_not_exists')\r\n    api.send('output','Stop')\r\n        \r\n        \r\n\r\n\r\ndef on_input(data1):\r\n    time.sleep(11)\r\n    file_exists = exists(\"/vrep/vflow/trigger_C_SALESDOCITMPRCGELMNTDEX_1.txt\")\r\n    if file_exists:\r\n        api.send('output2','file_exists_2')\r\n    else:\r\n            \r\n        api.send('output2','file_not_exists_2')\r\n        api.send('output','Stop')\r\n\r\n    \r\n    \"\"\"\r\n    while file_exists :\r\n        time.sleep(10)\r\n        file_exists = exists(\"/vrep/vflow/trigger_I_CUS.txt\")\r\n        if file_exists:\r\n            api.send('output2','file_exists_2')\r\n        else:\r\n            \r\n            api.send('output2','file_not_exists_2')\r\n        \r\n        \r\n    api.send('output','Stop')\r\n    \"\"\"\r\napi.set_port_callback('input', on_input)"
				},
				"additionalinports": [
					{
						"name": "input",
						"type": "string"
					}
				],
				"additionaloutports": [
					{
						"name": "output",
						"type": "string"
					},
					{
						"name": "output2",
						"type": "string"
					}
				]
			}
		},
		"wiretap2": {
			"component": "com.sap.util.wiretap",
			"metadata": {
				"label": "Wiretap",
				"x": 260.99999809265137,
				"y": 172,
				"height": 80,
				"width": 120,
				"generation": 1,
				"ui": "dynpath",
				"config": {}
			}
		}
	},
	"groups": [
		{
			"name": "group2",
			"nodes": [
				"snowflake1"
			],
			"metadata": {
				"description": "Group"
			},
			"tags": {
				"snwflk": ""
			}
		}
	],
	"connections": [
		{
			"metadata": {
				"points": "457.99999713897705,80 577.9999957084656,80 577.9999957084656,205.99999976158142 621.9999952316284,205.99999976158142"
			},
			"src": {
				"port": "stop",
				"process": "snowflake1"
			},
			"tgt": {
				"port": "stop",
				"process": "graphterminator1"
			}
		},
		{
			"metadata": {
				"points": "196.99999904632568,72 224.99999856948853,72 224.99999856948853,74.5 300.9999976158142,74.5 300.9999976158142,71 328.99999713897705,71"
			},
			"src": {
				"port": "outMessageData",
				"process": "sapabapoperator1"
			},
			"tgt": {
				"port": "input1",
				"process": "snowflake1"
			}
		},
		{
			"metadata": {
				"points": "196.99999904632568,183 224.99999856948853,183 224.99999856948853,85.5 300.9999976158142,85.5 300.9999976158142,89 328.99999713897705,89"
			},
			"src": {
				"port": "output",
				"process": "python3operator1"
			},
			"tgt": {
				"port": "input2",
				"process": "snowflake1"
			}
		},
		{
			"metadata": {
				"points": "196.99999904632568,201 224.99999856948853,201 224.99999856948853,212 255.99999809265137,212"
			},
			"src": {
				"port": "output2",
				"process": "python3operator1"
			},
			"tgt": {
				"port": "in",
				"process": "wiretap2"
			}
		},
		{
			"metadata": {
				"points": "457.99999713897705,62 485.9999966621399,62 485.9999966621399,69 593.9999957084656,69 593.9999957084656,85.99999976158142 621.9999952316284,85.99999976158142"
			},
			"src": {
				"port": "output",
				"process": "snowflake1"
			},
			"tgt": {
				"port": "in",
				"process": "wiretap1"
			}
		},
		{
			"metadata": {
				"points": "457.99999713897705,98 485.9999966621399,98 485.9999966621399,91 561.9999957084656,91 561.9999957084656,279.99999952316284 224.99999856948853,279.99999952316284 224.99999856948853,259.99999952316284 12,259.99999952316284 12,192 67.99999904632568,192"
			},
			"src": {
				"port": "output2",
				"process": "snowflake1"
			},
			"tgt": {
				"port": "input",
				"process": "python3operator1"
			}
		}
	],
	"inports": {},
	"outports": {},
	"metadata": {
		"generation": 1
	}
}