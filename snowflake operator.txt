{
	"component": "com.sap.system.python3Operator.v2",
	"description": "Snowflake Gen2",
	"inports": [],
	"outports": [],
	"tags": {},
	"subenginestags": {},
	"config": {
		"$type": "http://sap.com/vflow/Snowflake Gen2.configSchema.json",
		"errorHandling": "{\"type\":\"terminate on error\"}",
		"script": "import pandas as pd\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\nfrom io import StringIO\nimport csv\nimport json\n\n\ndef on_input(msg_id, header, data):\n    conn_sf = snowflake.connector.connect(\n    account =  api.config.Account,\n    user = api.config.Username,\n    warehouse = api.config.Warehouse,\n    password = api.config.Password,\n    database = api.config.Database,\n    schema = api.config.Schema)\n    \n    \n        \n        \n    #header_dict = dict(zip(['index','max_index','num_rows','periodicity'],list(header.values())[0]))\n    header_dict = dict(zip(['index'],list(header.values())[0]))\n    tbl = data.get()\n    x=header['com.sap.headers.batch']\n    y=x[1]\n    if x[1]==False:\n        #api.outputs.log.publish(f\"Batch: {header_dict['index']} Num Records: {len(tbl)}\") \n        tbl_info = api.type_context.get_vtype(tbl.type_ref)\n        col_names = list(tbl_info.columns.keys())\n        df = pd.DataFrame(tbl.body,columns = col_names,dtype=str)\n        df.rename(columns = {'/1DH/OPERATION':'OPERATION'}, inplace = True)\n        df.insert(0, 'TIMESTAMP', pd.to_datetime('now').replace(microsecond=0))\n        df['TIMESTAMP'] = df['TIMESTAMP'].dt.tz_localize('UTC')\n        temp_cols=df.columns.tolist()\n        new_cols=temp_cols[1:] + temp_cols[0:1]\n        df=df[new_cols]\n        table=api.config.Snowflake_Table_Name\n        with conn_sf as db_conn_sf, db_conn_sf.cursor() as db_cursor_sf:\n            inserted_rows = write_pandas(conn = db_conn_sf, df = df,table_name = table, quote_identifiers = False)\n        api.outputs.log.publish(str(inserted_rows))\n    else:\n        api.outputs.log2.publish(str('no data'))\n        \n        \napi.set_port_callback(\"input1\", on_input)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# For more information about the Python3Operator, drag it to the graph canvas,\n# right click on it, and click on \"Open Documentation\".\n\n# To uncomment the snippets below you can highlight the relevant lines and\n# press Ctrl+/ on Windows and Linux or Cmd+/ on Mac.\n\n# # Basic Example 1: Count inputs so far and send on output port (port type\n# com.sap.core.string)\n# # When using the snippet below make sure you create an output port of type\n# #string\n# counter = 0\n#\n# def on_input(msg_id, header, body):\n#     global counter\n#     counter += 1\n#     api.outputs.output.publish(str(counter))\n#\n# api.set_port_callback(\"input\", on_input)\n\n\n# # Basic Example 2: Read incoming table as stream and send it\n# as stream as well to the output port (any table port type),\n# # When using the snippet below make sure you create an input and output\n# # port of table type\n#\n# import threading\n# chunk_size = 10\n# \n# # Since this is run in a different thread, exceptions on it will not\n# # trigger any action. Alternatives are using `api.propagate_exception\n# # or sending through a queue to the operator main thread (the callback one).\n# def process_batch(body):\n#     try:\n#         reader = body.get_reader()\n#         # These two line is needed if the output port is of type-id \"*\" (dynamic)\n#         # It specifies the type of the dynamic table output which is\n#         # in this case the type of the given input table\n#         table = reader.read(chunk_size)\n#         api.outputs.output.set_dynamic_type(table.type_ref)\n#         # This allows creating one output stream per thread,\n#         # each being able to send data in parallel.\n#         msg_id, writer = api.outputs.output.with_writer()\n#         # Send the first chunk before running the while loop\n#         writer.write(table)\n#         while True:\n#             table = reader.read(chunk_size)\n#             # When the stream is closed, len(table) < expected.\n#             # If -1 was passed, read would wait for stream to close\n#             if len(table) <= 0:\n#                 api.logger.info('End of table')\n#                 break\n#             \n#             writer.write(table)\n#         writer.close()\n#     except Exception as ex:\n#         api.propagate_exception(ex)\n# \n# def on_input(msg_id, header, body):\n#     # Since each input thriggers a thread, it is possible to have\n#     # multiple actions happening in parallel.\n#     threading.Thread(target=process_batch, args=[body]).start()\n# \n# api.set_port_callback(\"input\", on_input)\n\n\n# # Basic Example 3: State Management support, more details at the operator\n# # documentation.\n# # This operator will accumulate all the numeric values it gets on the input\n# # and it will recover when the graph crashes with the accumulated value stored as state of this operator\n# # When using the snippet below make sure you create input of the com.sap.core.int64\n# # and output port of the com.sap.core.string type.\n# # You can use Basic Example 4 as a generator operator for the integer inputs of this example\n# # Also make sure to start the graph with State Management enabled (via Run As..)\n#\n# # Statemanagement needs to be only implemented for operators which need to store a state\n# # that influences the processing of the input messages (like in this example adding the accumulated value to the output).\n# # If you can process the input message independently of any state variable\n# # (e.g. the output is always the same for the same input message after a graph recovery)\n# # then you don't need to implement these methods as your operator is considered stateless.\n# import pickle\n# import random\n# import time\n#\n# # Internal operator state\n# acc = 0\n#\n# def on_input(msg_id, header, body):\n#     global acc\n#     v = body.get()\n#     acc += v\n#     if random.randint(1,101) % 100 == 0:\n#         # Enforce a crash with 1% chance to enforce a recovery restart\n#         # But sleep 10 seconds to ensure that you can view the results of the output\n#         time.sleep(10)\n#         raise Exception(\"crashed with 1% chance. Feel free to adjust the chance based on your needs.\")\n#     api.outputs.output.publish(\"%d: %d\" % (v, acc))\n#\n# api.set_port_callback(\"input\", on_input)\n#\n# # It is required to have `is_stateful` set, but since this operator\n# # script does not define a generator no information about output port is passed.\n# # More details in the operator documentation.\n#\n# api.set_initial_snapshot_info(api.InitialProcessInfo(is_stateful=True))\n#\n# def serialize(epoch):\n#     return pickle.dumps(acc)\n#\n# api.set_serialize_callback(serialize)\n#\n# def restore(epoch, state_bytes):\n#     global acc\n#     acc = pickle.loads(state_bytes)\n#\n# api.set_restore_callback(restore)\n#\n# def complete_callback(epoch):\n#     api.logger.info(f\"epoch {epoch} is completed!!!\")\n#\n# api.set_epoch_complete_callback(complete_callback)\n# \n# # Basic Example 4: Prestart\n# # When using the snippet below make sure you create an output port of type\n# # com.sap.core.int64\n# counter = 0\n#\n# def gen():\n#     global counter\n#     for i in range(10000):\n#         api.outputs.output.publish(counter)\n#         counter += 1\n#\n# api.set_prestart(gen)\n\n# # Basic Example 5: Timer\n# # When using the snippet below make sure you create an output port of type\n# # com.sap.core.binary\n# import os\n# from io import BytesIO\n# import datetime\n#\n# # Function called when operator handling mode is set to `retry`\n# # (more details at the operator documentation)\n# def custom_response_callback(msg_id, ex):\n#     if ex:\n#         api.logger.error(\"Error when publishing %s: %s\" % (str(msg_id), str(ex)))\n#\n#\n# def time_callback():\n#     dummy_bytes = os.urandom(20)\n#     dummy_binary = BytesIO(dummy_bytes)\n#     dummy_headers = {\n#         # header (structure) type-id: api.Record([list of structure fields])\n#         \"com.sap.headers.file\": api.Record([\n#             \"dummyConnection\", # connection\n#             \"dummy/Path\", # path\n#             len(dummy_bytes), # size\n#             False, #isDir\n#             datetime.datetime.now().isoformat() # modTime\n#         ])\n#     }\n#     # Send all binary data at once to the output, if only the first\n#     # 10 bytes were to be sent, `n` = 10\n#     msg_id = api.outputs.output.publish(dummy_binary, -1,\n#                                         header=dummy_headers,\n#                                         response_callback=custom_response_callback)\n#     # Controls the time until the next call to time_callback\n#     return 1\n#\n# api.add_timer(time_callback)\n\n# # Basic Example 6: Shutdown\n# # The shutdown function will be called when the operator receives a stop signal\n# # In this example it prints a simple information in the trace logs with the current message counter\n# # When using the snippet below make sure you create an input port of an arbitrary type and stop the graph\n# # You can check the graph diagnostics / traces of the graph for the shutdown message when the graph stopped.\n# counter = 0\n#\n# def on_input(msg_id, header, body):\n#     global counter\n#     counter += 1\n#\n# api.set_port_callback(\"input\", on_input)\n#\n# def shutdown():\n#     api.logger.info(\"shutdown: %d\" % counter)\n#\n# api.set_shutdown(shutdown)\n",
		"Account": "Kraft.east-us-2.privatelink",
		"Username": "khc_sapdi_conn_user_dev",
		"Password": "9xXC=lS_3/_^4}v6",
		"Database": "DBS_GENERIC_NONSECURE_SBX",
		"Schema": "ingestion",
		"Warehouse": "dev_cloud_analytics_platform",
		"Role": "DEV_KHC_SAPDI"
	},
	"versionStatus": "active",
	"icon": "snowflake-o"
}