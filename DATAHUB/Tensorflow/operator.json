{
	"component": "com.sap.system.python3Operator",
	"description": "test script",
	"inports": [
		{
			"name": "input1",
			"type": "message"
		},
		{
			"name": "input2",
			"type": "message"
		}
	],
	"outports": [
		{
			"name": "output",
			"type": "string"
		},
		{
			"name": "output2",
			"type": "message"
		}
	],
	"tags": {},
	"subenginestags": {},
	"config": {
		"$type": "http://sap.com/vflow/test_script.configSchema.json",
		"script": "from operators.test_script.function import foo\n\nimport pandas as pd\nimport io\nimport datetime\n\nfrom functools import partial\n\nimport multiprocessing\nfrom multiprocessing import get_context, cpu_count, Pool\n\n\n#def on_input(msg1, msg2):\ndef gen():\n    \n    GROUPBY_LEVEL = ['retailer', 'state', 'brand', 'ppg']\n    TARGET = \"FillMean\"\n    DATE_COLUMN = \"week_ending_date\"\n    # Add additional regressors to the forecast\n    ADDITIONAL_REGRESSORS = ['food_cpi_nat_mth', 'snap_cost_st_mth']\n    REGRESSOR_LAG = {'snap_cost_st_mth': 1}\n    \n    # Keep track of data throughout run\n    data = []\n    excluded = []\n    df_final_out = pd.DataFrame()\n    count=0\n    try_count = 0\n\n    # Establish training/test windows\n    TRAIN_START = pd.to_datetime('2019-01-01').date()\n    TRAIN_END = pd.to_datetime('2020-06-30').date()\n    TEST_START = pd.to_datetime('2020-09-30').date()\n    TEST_END = pd.to_datetime('2020-12-31').date()\n    # To speed up running, set to desired sample size if not set to 0\n    SAMPLE = 0\n    # Set logistic growth function cap\n    CAP_PERCENTILE = 95\n    # Future Period\n    FUTURE_PERIOD = 25\n    # model parameters\n    OPTIM_PARAM = {\"growth\": \"logistic\", \"seasonality_prior_scale\": 0.1}\n    \n\n    def custom_fillna(series):\n        if series.dtype is pd.np.dtype(float):\n            return series.fillna(0)\n        elif series.dtype is pd.np.dtype('int32'):\n            return series.fillna(0)\n        elif series.dtype is pd.np.dtype('int64'):\n            return series.fillna(0)\n        elif series.dtype is pd.np.dtype(str):\n            return series.fillna(0)  \n        elif series.dtype is pd.np.dtype(object):\n            return series.fillna('')  \n        else:\n            return series\n\n    \n    # Obtain data\n    # input table from SEP_COVIDEXT.Z_SEP.AnalyticalModels.SCM.DemandForecasting.CovidExternal::TA_SCM_COVID_FULL_SAMPLE_TRAIN\n    # format data frame as per data type in source table\n    \n    #df_sample_train = pd.read_csv(io.StringIO(msg1.body), sep=\",\")\n    #df =  pd.read_json(io.StringIO(msg2))\n    df =  pd.read_csv('/vrep/csvs/scm_demand_sensing_masked_data_imputed_v3_short.csv')\n    df = df[['week_ending_date', 'retailer', 'state', 'business', 'category', 'brand', 'ppg', 'pos_qty_ty', 'pos_dollar_ty', 'FillMean']]\n    \n    # break according to api.multiplicity and api.multiplicity_index\n    batch_size = int(df.shape[0]/api.multiplicity)\n    begin_batch = api.multiplicity_index*batch_size\n    end_batch = (api.multiplicity_index+1)*batch_size\n    \n    df = df.iloc[begin_batch:end_batch]\n    \n    #checking null values and replace accordingly\n    df = df.apply(custom_fillna)\n    df['week_ending_date'] =  df['week_ending_date'].astype(str)\n    \n    #checking null values and replace accordingly\n    #df_add_external =  pd.read_json(io.StringIO(msg1))\n    df_add_external =  pd.read_csv('/vrep/csvs/external_merged_weekly_modeling_source.csv')\n    df_add_external = df_add_external[['date', 'state', 'state_initial', 'AT_adj', 'food_cpi_nat_mth', 'snap_cost_st_mth', 'allbed_mean', 'confirmed_infections', 'deaths_mean', 'est_infections_mean', 'mobility_composite_wors', 'states_on_stay_home', 'states_on_travel_limit', 'states_on_any_business', 'states_on_all_non-ess_business', 'states_on_any_gathering_restrict', 'states_on_educational_fac']] \n    \n    api.send('output', str(df.shape))\n    df_add_external = df_add_external.apply(custom_fillna)\n    df_add_external['date'] =  df_add_external['date'].astype(str)\n  \n    def get_end_date_from_week(year,week,day):\n\n        #Calculates first day and last day of week, given a year and week\n        first_day = datetime.datetime.strptime(f'{year}-W{int(week )- 1}-1', '%Y-W%W-%w').date()\n        last_day = first_day + datetime.timedelta(days=day)\n        return last_day\n    \n    def get_week_number_from_end_date(date_obj):\n        #Calculates week number in year given a date.\n        week_number = date_obj.isocalendar()[1]\n        return week_number   \n    \n    def data_imputation(df):\n        # Fill in missing week numbers\n        df['week_of_year'] = df[DATE_COLUMN].apply(lambda x: get_week_number_from_end_date(x))\n        df[DATE_COLUMN] = pd.to_datetime(df[DATE_COLUMN]).dt.date\n        grouped = df.groupby(GROUPBY_LEVEL)\n        data1 =  grouped.values.tolist()\n\n        subset_list = []\n        for name, group in grouped:\n            subset_df = group.copy()\n            subset_df = subset_df.set_index(DATE_COLUMN)\n            # Imputation approach using mean value\n            subset_df = subset_df.assign(imputed_qty=subset_df[TARGET].fillna(subset_df[TARGET].mean()))\n            subset_df = subset_df.reset_index()\n            subset_list.append(subset_df)\n        imputed_df = pd.concat(subset_list)\n        #imputed_df.to_csv(DATA_PATH+'processed/imputed_data.csv', index=False)\n        return imputed_df\n\n    def add_external_data(df, df_add):\n        #Takes client data and external data\n        \n\n        df_add = df_add[['date', 'state_initial'] + ADDITIONAL_REGRESSORS]\n        df_add.rename(columns={'date': 'week_ending_date', 'state_initial': 'state'}, inplace=True)\n        if REGRESSOR_LAG != {}:\n            for k, v in REGRESSOR_LAG.items():\n                df_add[k] = df_add.groupby('state')[k].shift(v)\n        df = pd.merge(df, df_add, on=['week_ending_date', 'state'], how='left')\n        df_add[DATE_COLUMN] = pd.to_datetime(df_add[DATE_COLUMN]).dt.date\n        df_add.rename(columns={DATE_COLUMN: 'ds'}, inplace=True)\n        return df, df_add\n\n    def select_data(df):\n        if ADDITIONAL_REGRESSORS:\n            df = df[[DATE_COLUMN] + GROUPBY_LEVEL + ADDITIONAL_REGRESSORS + [TARGET]]\n        else:\n            df = df[[DATE_COLUMN] + GROUPBY_LEVEL + [TARGET]]\n        print('Selected data columns.')\n\n        return df\n\n    def select_sample(df):\n        df_sum = df.groupby(GROUPBY_LEVEL)[[TARGET]].sum().reset_index().rename(columns={TARGET: 'total_qty'})\n        top_100 = df_sum.nlargest(columns='total_qty', n=SAMPLE)\n        top_100 = top_100.drop(['total_qty'], axis=1)\n        df = pd.merge(top_100, df, how='inner', on=GROUPBY_LEVEL)\n        print('Chose top {} samples.'.format(SAMPLE))\n        return df\n    \n    def load_holiday_calendar():\n        # Builds a holiday calendar.\n        # New year's day\n        newyear = pd.DataFrame({\n        'holiday': 'newyear',\n        'ds': pd.to_datetime(['2019-01-01','2020-01-01']),\n        })\n        # Martin Luther King Jr. Day\n        MLK_day = pd.DataFrame({\n        'holiday': 'MLK_day',\n        'ds': pd.to_datetime(['2019-01-21','2020-01-20']),\n        })\n        # March Madness\n        march_madness = pd.DataFrame({\n        'holiday': 'march_madness',\n        'ds': pd.to_datetime(['2018-03-24','2018-03-31','2019-03-30','2019-04-06','2020-03-28','2020-04-04', '2021-03-27','2021-04-03']),\n        })\n        # Superbowl\n        superbowls = pd.DataFrame({\n        'holiday': 'superbowl',\n        'ds': pd.to_datetime(['2018-01-27','2018-02-03', '2019-01-26','2019-02-02', '2020-01-25','2020-02-01','2021-01-30','2021-02-06']),\n        })\n        # Lent\n        lent = pd.DataFrame({\n        'holiday': 'lent',\n        'ds': pd.to_datetime(['2018-02-17', '2018-02-24','2018-03-03','2018-03-10','2018-03-17','2018-03-24',\n                            '2019-03-09', '2019-03-16','2019-03-23','2019-03-30','2019-04-06','2019-04-13',\n                            '2020-02-29', '2020-03-07', '2020-03-14','2020-03-21','2020-03-28','2020-04-04',\n                            '2021-02-20', '2021-02-27', '2021-03-06','2021-03-13','2021-03-20','2021-03-27']),\n        })\n        # Easter (Wednesday â€“ Easter Friday)\n        easter = pd.DataFrame({\n        'holiday': 'easter',\n        'ds': pd.to_datetime(['2018-03-31', '2019-04-20', '2020-04-11','2021-04-03']),\n        })\n        # Memorial day\n        memorial_day = pd.DataFrame({\n        'holiday': 'memorial_day',\n        'ds': pd.to_datetime(['2019-05-27', '2020-05-25']),\n        })\n        # Independence day\n        indep_day = pd.DataFrame({\n        'holiday': 'indep_day',\n        'ds': pd.to_datetime(['2019-07-04', '2020-07-03']),\n        })\n        # Labor day\n        labor_day = pd.DataFrame({\n        'holiday': 'indep_day',\n        'ds': pd.to_datetime(['2019-09-02', '2020-09-07']),\n        })\n        # Halloween\n        halloween = pd.DataFrame({\n        'holiday': 'halloween',\n        'ds': pd.to_datetime(['2018-10-27', '2019-10-26', '2020-10-31','2021-10-30']),\n        })\n        # Veteran's day\n        veteran_day = pd.DataFrame({\n        'holiday': 'veteran_day',\n        'ds': pd.to_datetime(['2019-11-11', '2020-11-11']),\n        })\n        # Thanksgiving\n        thanksgiving = pd.DataFrame({\n        'holiday': 'thanksgiving',\n        'ds': pd.to_datetime(['2019-11-28', '2020-11-26']),\n        })\n        # Christmas\n        Christmas = pd.DataFrame({\n        'holiday': 'thanksgiving',\n        'ds': pd.to_datetime(['2019-12-25', '2020-12-25']),\n        })\n\n        holidays_df = pd.concat((newyear, MLK_day, march_madness, superbowls, lent, easter, memorial_day, indep_day, labor_day, halloween, veteran_day, thanksgiving, Christmas))\n        return holidays_df\n\n    def get_week_day(df):\n        df['ds'] = pd.to_datetime(df['ds']).dt.date\n        return df['ds'].iloc[0].weekday()\n\n    def custom_holidays(week_day):\n        custom_holidays = load_holiday_calendar()\n        custom_holidays['week_no'] = custom_holidays['ds'].apply(lambda x: get_week_number_from_end_date(x))\n        custom_holidays['year'] = custom_holidays['ds'].apply(lambda x: int(x.strftime('%Y')))\n        custom_holidays['week_ending_date'] = custom_holidays.apply(\n            lambda x: get_end_date_from_week(x['year'], x['week_no'], week_day), 1)\n        custom_holidays.rename(columns={'ds': 'date', 'week_ending_date': 'ds'}, inplace=True)\n        custom_holidays = custom_holidays[['ds', 'holiday']]\n\n        return custom_holidays\n\n    # def plot_mape(stats_df):\n    #     plt.style.use('ggplot')\n    #     first_edge, last_edge = stats_df['mape'].min(), stats_df['mape'].max()\n\n    #     n_equal_bins = 60\n    #     bin_edges = np.linspace(start=first_edge, stop=last_edge, num=n_equal_bins + 1, endpoint=True)\n\n    #     # Creating histogram\n    #     fig, ax = plt.subplots(figsize =(8, 4))\n    #     ax.hist(stats_df['mape'], bins = bin_edges,  color = (0.5,0.1,0.5,0.6))\n\n    #     plt.title('MAPE distribution of forecast results.')\n\n    #     # Save plot\n    #     plt.savefig(DATA_PATH+'mape_plot.png')\n\n    def mean_absolute_percentage_error(y_true, y_pred):\n        y_true, y_pred = np.array(y_true), np.array(y_pred)\n        return np.mean(np.abs((y_true - y_pred)/ y_true)) * 100\n    \n\n    df, df_add_new = add_external_data(df, df_add_external)\n    df_add_new['ds'] =  df_add_new['ds'].astype(str)\n    #data =  df.values.tolist()\n\n    # Track how long end-to-end modeling takes\n    #load data, date should be in first column\n    df[DATE_COLUMN] = pd.to_datetime(df[DATE_COLUMN]).dt.date\n    df[DATE_COLUMN] =  df[DATE_COLUMN].astype(str) #Convert Date to String for DI\n    # Load Additional Data\n    \n    # Get relevant columns\n    df = select_data(df)\n    #data =  df.values.tolist()\n\n    #get sample, if number provided, else run on full set\n    if SAMPLE:\n        df = select_sample(df)\n    df.rename(columns={DATE_COLUMN: 'ds', TARGET: 'y'}, inplace=True)\n\n    #get Weekday\n    week_day = get_week_day(df)\n    #create custom holiday\n    cust_df_new = custom_holidays(week_day)\n    cust_df_new['ds'] =  cust_df_new['ds'].astype(str)\n    \n\n    df_add_new['ds'] =  df_add_new['ds'].astype(str)\n    \n    #train, forecast, and get results\n    final_data = []\n    excluded_groups = []\n    # Rename to prophet's requirements\n    df.rename(columns={DATE_COLUMN: 'ds', TARGET: 'y'}, inplace=True)\n    # Group data to build individual models at each level\n    #grouped= pd.DataFrame()#Force grouped to a dataframe\n    #grouped1= pd.DataFrame()\n    grouped = df.groupby(GROUPBY_LEVEL) #To check if groupby() is working\n    grouped_ak = df.groupby(GROUPBY_LEVEL).count() #by AK\n    data = grouped_ak.values.tolist()\n\n    data2 = grouped_ak.values.tolist()\n\n    count=0\n    lst=[]\n    api.send('output', str(len(grouped.groups)))\n    \n    pool = multiprocessing.Pool(processes=8)\n    with get_context(\"spawn\").Pool() as pool:\n        res = pool.map(partial(foo, grouped=grouped, cust_df_new=cust_df_new, df_add_new=df_add_new), list(grouped.groups.keys())[:200], chunksize=20)\n        pool.close()\n        pool.join()\n    \n    api.send('output', str(len(res)))\n    api.send('output', str(res))\n        \napi.add_generator(gen)\n\n#api.set_port_callback([\"input1\",\"input2\"], on_input)\n    "
	},
	"versionStatus": "active",
	"icon": "puzzle-piece"
}