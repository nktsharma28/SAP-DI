{
	"properties": {},
	"description": "",
	"processes": {
		"testscript1": {
			"component": "test_script",
			"metadata": {
				"label": "test script",
				"x": 29,
				"y": 80,
				"height": 80,
				"width": 120,
				"extensible": true,
				"config": {
					"script": "from operators.test_script.function import foo\n\nimport pandas as pd\nimport io\nimport datetime\nimport pickle\n\nfrom functools import partial\n\nimport multiprocessing\nfrom multiprocessing import get_context, cpu_count, Pool\nimport random\n\ndef on_input(message):\n    if(message.body and message.attributes['table']):\n        #Column names are hardcoded because of column tables don't match exactly names used on the script below\n        df = pd.DataFrame(message.body, columns=[\"week_ending_date\",\"retailer\",\"state\",\"business\",\"category\",\"brand\",\"ppg\",\"week_of_year\",\"pos_qty_ty\",\"pos_dollar_ty\",\"FillMean\"])\n        api.send(\"output\", str(len(df.index)))\n    else:\n        return\n\n    GROUPBY_LEVEL = ['retailer', 'state', 'brand', 'ppg']\n    TARGET = \"FillMean\"\n    DATE_COLUMN = \"week_ending_date\"\n    # Add additional regressors to the forecast\n    ADDITIONAL_REGRESSORS = ['food_cpi_nat_mth', 'snap_cost_st_mth']\n    REGRESSOR_LAG = {'snap_cost_st_mth': 1}\n    \n    # Keep track of data throughout run\n    data = []\n    excluded = []\n    df_final_out = pd.DataFrame()\n    count=0\n    try_count = 0\n\n    # Establish training/test windows\n    TRAIN_START = pd.to_datetime('2019-01-01').date()\n    TRAIN_END = pd.to_datetime('2020-06-30').date()\n    TEST_START = pd.to_datetime('2020-09-30').date()\n    TEST_END = pd.to_datetime('2020-12-31').date()\n    # To speed up running, set to desired sample size if not set to 0\n    SAMPLE = 0\n    # Set logistic growth function cap\n    CAP_PERCENTILE = 95\n    # Future Period\n    FUTURE_PERIOD = 25\n    # model parameters\n    OPTIM_PARAM = {\"growth\": \"logistic\", \"seasonality_prior_scale\": 0.1}\n    \n\n    def custom_fillna(series):\n        if series.dtype is pd.np.dtype(float):\n            return series.fillna(0)\n        elif series.dtype is pd.np.dtype('int32'):\n            return series.fillna(0)\n        elif series.dtype is pd.np.dtype('int64'):\n            return series.fillna(0)\n        elif series.dtype is pd.np.dtype(str):\n            return series.fillna(0)  \n        elif series.dtype is pd.np.dtype(object):\n            return series.fillna('')  \n        else:\n            return series\n\n    \n    # Obtain data\n    # input table from SEP_COVIDEXT.Z_SEP.AnalyticalModels.SCM.DemandForecasting.CovidExternal::TA_SCM_COVID_FULL_SAMPLE_TRAIN\n    # format data frame as per data type in source table\n    \n    #df_sample_train = pd.read_csv(io.StringIO(msg1.body), sep=\",\")\n    #df =  pd.read_json(io.StringIO(msg2))\n    df =  pd.read_csv('/vrep/csvs/scm_demand_sensing_masked_data_imputed_v3.csv')\n    df = df[['week_ending_date', 'retailer', 'state', 'business', 'category', 'brand', 'ppg', 'pos_qty_ty', 'pos_dollar_ty', 'FillMean']]\n    \n    # break according to api.multiplicity and api.multiplicity_index\n    batch_size = int(df.shape[0]/api.multiplicity)\n    begin_batch = api.multiplicity_index*batch_size\n    end_batch = (api.multiplicity_index+1)*batch_size\n    \n    df = df.iloc[begin_batch:end_batch]\n    \n    #checking null values and replace accordingly\n    df = df.apply(custom_fillna)\n    df['week_ending_date'] =  df['week_ending_date'].astype(str)\n    \n    #checking null values and replace accordingly\n    #df_add_external =  pd.read_json(io.StringIO(msg1))\n    df_add_external =  pd.read_csv('/vrep/csvs/external_merged_weekly_modeling_source.csv')\n    df_add_external = df_add_external[['date', 'state', 'state_initial', 'AT_adj', 'food_cpi_nat_mth', 'snap_cost_st_mth', 'allbed_mean', 'confirmed_infections', 'deaths_mean', 'est_infections_mean', 'mobility_composite_wors', 'states_on_stay_home', 'states_on_travel_limit', 'states_on_any_business', 'states_on_all_non-ess_business', 'states_on_any_gathering_restrict', 'states_on_educational_fac']] \n    \n    api.send('output', str(df.shape))\n    df_add_external = df_add_external.apply(custom_fillna)\n    df_add_external['date'] =  df_add_external['date'].astype(str)\n  \n    def get_end_date_from_week(year,week,day):\n\n        #Calculates first day and last day of week, given a year and week\n        first_day = datetime.datetime.strptime(f'{year}-W{int(week )- 1}-1', '%Y-W%W-%w').date()\n        last_day = first_day + datetime.timedelta(days=day)\n        return last_day\n    \n    def get_week_number_from_end_date(date_obj):\n        #Calculates week number in year given a date.\n        week_number = date_obj.isocalendar()[1]\n        return week_number   \n    \n    def data_imputation(df):\n        # Fill in missing week numbers\n        df['week_of_year'] = df[DATE_COLUMN].apply(lambda x: get_week_number_from_end_date(x))\n        df[DATE_COLUMN] = pd.to_datetime(df[DATE_COLUMN]).dt.date\n        grouped = df.groupby(GROUPBY_LEVEL)\n        data1 =  grouped.values.tolist()\n\n        subset_list = []\n        for name, group in grouped:\n            subset_df = group.copy()\n            subset_df = subset_df.set_index(DATE_COLUMN)\n            # Imputation approach using mean value\n            subset_df = subset_df.assign(imputed_qty=subset_df[TARGET].fillna(subset_df[TARGET].mean()))\n            subset_df = subset_df.reset_index()\n            subset_list.append(subset_df)\n        imputed_df = pd.concat(subset_list)\n        #imputed_df.to_csv(DATA_PATH+'processed/imputed_data.csv', index=False)\n        return imputed_df\n\n    def add_external_data(df, df_add):\n        #Takes client data and external data\n        \n\n        df_add = df_add[['date', 'state_initial'] + ADDITIONAL_REGRESSORS]\n        df_add.rename(columns={'date': 'week_ending_date', 'state_initial': 'state'}, inplace=True)\n        if REGRESSOR_LAG != {}:\n            for k, v in REGRESSOR_LAG.items():\n                df_add[k] = df_add.groupby('state')[k].shift(v)\n        df = pd.merge(df, df_add, on=['week_ending_date', 'state'], how='left')\n        df_add[DATE_COLUMN] = pd.to_datetime(df_add[DATE_COLUMN]).dt.date\n        df_add.rename(columns={DATE_COLUMN: 'ds'}, inplace=True)\n        return df, df_add\n\n    def select_data(df):\n        if ADDITIONAL_REGRESSORS:\n            df = df[[DATE_COLUMN] + GROUPBY_LEVEL + ADDITIONAL_REGRESSORS + [TARGET]]\n        else:\n            df = df[[DATE_COLUMN] + GROUPBY_LEVEL + [TARGET]]\n        print('Selected data columns.')\n\n        return df\n\n    def select_sample(df):\n        df_sum = df.groupby(GROUPBY_LEVEL)[[TARGET]].sum().reset_index().rename(columns={TARGET: 'total_qty'})\n        top_100 = df_sum.nlargest(columns='total_qty', n=SAMPLE)\n        top_100 = top_100.drop(['total_qty'], axis=1)\n        df = pd.merge(top_100, df, how='inner', on=GROUPBY_LEVEL)\n        print('Chose top {} samples.'.format(SAMPLE))\n        return df\n    \n    def load_holiday_calendar():\n        # Builds a holiday calendar.\n        # New year's day\n        newyear = pd.DataFrame({\n        'holiday': 'newyear',\n        'ds': pd.to_datetime(['2019-01-01','2020-01-01']),\n        })\n        # Martin Luther King Jr. Day\n        MLK_day = pd.DataFrame({\n        'holiday': 'MLK_day',\n        'ds': pd.to_datetime(['2019-01-21','2020-01-20']),\n        })\n        # March Madness\n        march_madness = pd.DataFrame({\n        'holiday': 'march_madness',\n        'ds': pd.to_datetime(['2018-03-24','2018-03-31','2019-03-30','2019-04-06','2020-03-28','2020-04-04', '2021-03-27','2021-04-03']),\n        })\n        # Superbowl\n        superbowls = pd.DataFrame({\n        'holiday': 'superbowl',\n        'ds': pd.to_datetime(['2018-01-27','2018-02-03', '2019-01-26','2019-02-02', '2020-01-25','2020-02-01','2021-01-30','2021-02-06']),\n        })\n        # Lent\n        lent = pd.DataFrame({\n        'holiday': 'lent',\n        'ds': pd.to_datetime(['2018-02-17', '2018-02-24','2018-03-03','2018-03-10','2018-03-17','2018-03-24',\n                            '2019-03-09', '2019-03-16','2019-03-23','2019-03-30','2019-04-06','2019-04-13',\n                            '2020-02-29', '2020-03-07', '2020-03-14','2020-03-21','2020-03-28','2020-04-04',\n                            '2021-02-20', '2021-02-27', '2021-03-06','2021-03-13','2021-03-20','2021-03-27']),\n        })\n        # Easter (Wednesday â€“ Easter Friday)\n        easter = pd.DataFrame({\n        'holiday': 'easter',\n        'ds': pd.to_datetime(['2018-03-31', '2019-04-20', '2020-04-11','2021-04-03']),\n        })\n        # Memorial day\n        memorial_day = pd.DataFrame({\n        'holiday': 'memorial_day',\n        'ds': pd.to_datetime(['2019-05-27', '2020-05-25']),\n        })\n        # Independence day\n        indep_day = pd.DataFrame({\n        'holiday': 'indep_day',\n        'ds': pd.to_datetime(['2019-07-04', '2020-07-03']),\n        })\n        # Labor day\n        labor_day = pd.DataFrame({\n        'holiday': 'indep_day',\n        'ds': pd.to_datetime(['2019-09-02', '2020-09-07']),\n        })\n        # Halloween\n        halloween = pd.DataFrame({\n        'holiday': 'halloween',\n        'ds': pd.to_datetime(['2018-10-27', '2019-10-26', '2020-10-31','2021-10-30']),\n        })\n        # Veteran's day\n        veteran_day = pd.DataFrame({\n        'holiday': 'veteran_day',\n        'ds': pd.to_datetime(['2019-11-11', '2020-11-11']),\n        })\n        # Thanksgiving\n        thanksgiving = pd.DataFrame({\n        'holiday': 'thanksgiving',\n        'ds': pd.to_datetime(['2019-11-28', '2020-11-26']),\n        })\n        # Christmas\n        Christmas = pd.DataFrame({\n        'holiday': 'thanksgiving',\n        'ds': pd.to_datetime(['2019-12-25', '2020-12-25']),\n        })\n\n        holidays_df = pd.concat((newyear, MLK_day, march_madness, superbowls, lent, easter, memorial_day, indep_day, labor_day, halloween, veteran_day, thanksgiving, Christmas))\n        return holidays_df\n\n    def get_week_day(df):\n        df['ds'] = pd.to_datetime(df['ds']).dt.date\n        return df['ds'].iloc[0].weekday()\n\n    def custom_holidays(week_day):\n        custom_holidays = load_holiday_calendar()\n        custom_holidays['week_no'] = custom_holidays['ds'].apply(lambda x: get_week_number_from_end_date(x))\n        custom_holidays['year'] = custom_holidays['ds'].apply(lambda x: int(x.strftime('%Y')))\n        custom_holidays['week_ending_date'] = custom_holidays.apply(\n            lambda x: get_end_date_from_week(x['year'], x['week_no'], week_day), 1)\n        custom_holidays.rename(columns={'ds': 'date', 'week_ending_date': 'ds'}, inplace=True)\n        custom_holidays = custom_holidays[['ds', 'holiday']]\n\n        return custom_holidays\n\n    # def plot_mape(stats_df):\n    #     plt.style.use('ggplot')\n    #     first_edge, last_edge = stats_df['mape'].min(), stats_df['mape'].max()\n\n    #     n_equal_bins = 60\n    #     bin_edges = np.linspace(start=first_edge, stop=last_edge, num=n_equal_bins + 1, endpoint=True)\n\n    #     # Creating histogram\n    #     fig, ax = plt.subplots(figsize =(8, 4))\n    #     ax.hist(stats_df['mape'], bins = bin_edges,  color = (0.5,0.1,0.5,0.6))\n\n    #     plt.title('MAPE distribution of forecast results.')\n\n    #     # Save plot\n    #     plt.savefig(DATA_PATH+'mape_plot.png')\n\n    def mean_absolute_percentage_error(y_true, y_pred):\n        y_true, y_pred = np.array(y_true), np.array(y_pred)\n        return np.mean(np.abs((y_true - y_pred)/ y_true)) * 100\n    \n\n    df, df_add_new = add_external_data(df, df_add_external)\n    df_add_new['ds'] =  df_add_new['ds'].astype(str)\n    #data =  df.values.tolist()\n\n    # Track how long end-to-end modeling takes\n    #load data, date should be in first column\n    df[DATE_COLUMN] = pd.to_datetime(df[DATE_COLUMN]).dt.date\n    df[DATE_COLUMN] =  df[DATE_COLUMN].astype(str) #Convert Date to String for DI\n    # Load Additional Data\n    \n    # Get relevant columns\n    df = select_data(df)\n    #data =  df.values.tolist()\n\n    #get sample, if number provided, else run on full set\n    if SAMPLE:\n        df = select_sample(df)\n    df.rename(columns={DATE_COLUMN: 'ds', TARGET: 'y'}, inplace=True)\n\n    #get Weekday\n    week_day = get_week_day(df)\n    #create custom holiday\n    cust_df_new = custom_holidays(week_day)\n    cust_df_new['ds'] =  cust_df_new['ds'].astype(str)\n    \n\n    df_add_new['ds'] =  df_add_new['ds'].astype(str)\n    \n    #train, forecast, and get results\n    final_data = []\n    excluded_groups = []\n    # Rename to prophet's requirements\n    df.rename(columns={DATE_COLUMN: 'ds', TARGET: 'y'}, inplace=True)\n    # Group data to build individual models at each level\n    #grouped= pd.DataFrame()#Force grouped to a dataframe\n    #grouped1= pd.DataFrame()\n    grouped = df.groupby(GROUPBY_LEVEL) #To check if groupby() is working\n    grouped_ak = df.groupby(GROUPBY_LEVEL).count() #by AK\n    data = grouped_ak.values.tolist()\n\n    data2 = grouped_ak.values.tolist()\n\n    count=0\n    lst=[]\n    api.send('output', str(len(grouped.groups)))\n    \n    pool = multiprocessing.Pool(processes=8)\n    with get_context(\"spawn\").Pool() as pool:\n        res = pool.map(partial(foo, grouped=grouped, cust_df_new=cust_df_new, df_add_new=df_add_new), list(grouped.groups.keys()), chunksize=435)\n        pool.close()\n        pool.join()\n    api.send('output', 'LINE313')    \n    api.send('output', str(res))\n    joined_df = pd.concat(res)\n    joined_df.reset_index(inplace=True)\n    api.send('output', str(joined_df.shape))\n    api.send('output2', api.Message(joined_df.to_json()))\n\napi.set_port_callback(\"table\", on_input)\n    \n"
				},
				"additionalinports": [
					{
						"name": "table",
						"type": "message.table"
					}
				]
			}
		},
		"terminal1": {
			"component": "com.sap.util.terminal",
			"metadata": {
				"label": "Terminal",
				"x": 289.99999809265137,
				"y": 12,
				"height": 80,
				"width": 120,
				"ui": "dynpath",
				"config": {}
			}
		},
		"python3operator1": {
			"component": "com.sap.system.python3Operator",
			"metadata": {
				"label": "Python3 Operator",
				"x": 289.99999809265137,
				"y": 132,
				"height": 80,
				"width": 120,
				"extensible": true,
				"config": {
					"script": "import pandas as pd\nimport pickle\n\ndf_final_out = pd.DataFrame()\ncounter = 0\n\ndef on_input(data):\n    global counter\n    global df_final_out\n    partial_df = pd.read_json(data.body)\n    \n    if partial_df.shape[0] > 0:\n        df_final_out = df_final_out.append(partial_df)\n\n    counter += 1\n    if counter == 10:\n        api.send(\"output\", str(df_final_out.shape))\n\napi.set_port_callback(\"input\", on_input)\n\n\n# # Basic Example 2: Count inputs so far and send on output port (port type int64)\n# # When using the snippet below make sure you create an output port of type int64\n# counter = 0\n#\n# def on_input(data):\n#     global counter\n#     counter += 1\n#     api.send(\"output\", counter)\n#\n# api.set_port_callback(\"input\", on_input)\n\n\n# # Basic Example 3: Identity operator.\n# # When using the snippet below make sure you create input and output ports of the same type.\n# def on_input(data):\n#     api.send(\"output\", data)\n#\n# api.set_port_callback(\"input\", on_input)\n\n\n# # Basic Example 4: Sum both inputs and output result.\n# # When using the snippet below make sure you create input and output ports of the same type and\n# # that the corresponding python types can be added with the `+` operator. Example of valid\n# # port types for this snippet are: string, int64, and float64.\n# def on_input(data1, data2):\n#     api.send(\"output\", data1 + data2)\n#\n# api.set_port_callback([\"input1\", \"input2\"], on_input)\n\n\n# # Generators\n# # When using the snippet below make sure you create an output port of type int64\n# counter = 0\n#\n# def gen():\n#     global counter\n#     for i in range(0, 3):\n#         api.send(\"output\", counter)\n#         counter += 1\n#\n# api.add_generator(gen)\n# api.add_generator(gen)  # Adding the generator twice will make the function be executed twice.\n\n\n# # Timer\n# # When using the snippet below make sure you create an output port of type int64\n# counter = 0\n#\n# def t1():\n#     global counter\n#     api.send(\"output\", counter)\n#     counter += 1\n#\n# api.add_timer(\"1s\", t1)\n\n# # Timer\n# # When using the snippet below make sure you create an output port of type string\n# counter = 0\n#\n# def t2():\n#     global counter\n#     api.send(\"output\", str(counter))\n#     counter += 1\n#\n# api.add_timer(\"1s\", t2)\n\n\n# # Shutdown\n# counter = 0\n#\n# def on_input(data):\n#     global counter\n#     counter += 1\n#\n# api.set_port_callback(\"input\", on_input)\n#\n# def shutdown1():\n#     print(\"shutdown1: %d\" % counter)\n#\n# def shutdown2():\n#     print(\"shutdown2: %d\" % counter)\n#\n# api.add_shutdown_handler(shutdown1)\n# api.add_shutdown_handler(shutdown2)\n"
				},
				"additionalinports": [
					{
						"name": "input",
						"type": "message"
					}
				],
				"additionaloutports": [
					{
						"name": "output",
						"type": "string"
					}
				]
			}
		},
		"graphterminator1": {
			"component": "com.sap.util.graphTerminator",
			"metadata": {
				"label": "Graph Terminator",
				"x": 474.99999713897705,
				"y": 72,
				"height": 80,
				"width": 120,
				"config": {}
			}
		},
		"readhanatable1": {
			"component": "com.sap.hana.readTable",
			"metadata": {
				"label": "Read HANA Table",
				"x": -282,
				"y": 80,
				"height": 80,
				"width": 120,
				"config": {
					"connection": {
						"configurationType": "Configuration Manager",
						"connectionID": "PAL_HANA"
					},
					"configMode": "Static (from configuration)",
					"tableName": "DATAHUB.SCM_DEMAND_SENSING_MASKED_DATA_IMPUTED_V3_CSV",
					"batching": "Fixed size",
					"batchSize": 30000
				}
			}
		}
	},
	"groups": [
		{
			"name": "group2",
			"nodes": [
				"testscript1"
			],
			"metadata": {
				"description": "Group"
			},
			"tags": {
				"scm": ""
			},
			"multiplicity": 10
		}
	],
	"connections": [
		{
			"metadata": {
				"points": "153,111 180.99999952316284,111 180.99999952316284,114.5 256.9999985694885,114.5 256.9999985694885,52 284.99999809265137,52"
			},
			"src": {
				"port": "output",
				"process": "testscript1"
			},
			"tgt": {
				"port": "in1",
				"process": "terminal1"
			}
		},
		{
			"metadata": {
				"points": "153,129 180.99999952316284,129 180.99999952316284,125.5 256.9999985694885,125.5 256.9999985694885,172 284.99999809265137,172"
			},
			"src": {
				"port": "output2",
				"process": "testscript1"
			},
			"tgt": {
				"port": "input",
				"process": "python3operator1"
			}
		},
		{
			"metadata": {
				"points": "413.99999809265137,172 441.9999976158142,172 441.9999976158142,112 469.99999713897705,112"
			},
			"src": {
				"port": "output",
				"process": "python3operator1"
			},
			"tgt": {
				"port": "stop",
				"process": "graphterminator1"
			}
		},
		{
			"metadata": {
				"points": "-158,111 -67,111 -67,138 24,138"
			},
			"src": {
				"port": "success",
				"process": "readhanatable1"
			},
			"tgt": {
				"port": "table",
				"process": "testscript1"
			}
		}
	],
	"inports": {},
	"outports": {}
}