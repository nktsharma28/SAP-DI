{
	"properties": {},
	"description": "ML_PTM - TEST_PIPELINE_COGNITIVE",
	"processes": {
		"python3operator1111": {
			"component": "com.sap.system.python3Operator",
			"metadata": {
				"label": "CongnitivePricing_Training",
				"x": 521.999997138977,
				"y": 80,
				"height": 80,
				"width": 120,
				"extensible": true,
				"config": {
					"script": "\n# coding: utf-8\n\n# In[ ]:\n\n\n#import Libraries\n\nfrom __future__ import division\nimport pandas as pd\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.ensemble import GradientBoostingRegressor\n#import xgboost as xgb\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom pandas import Series, DataFrame\nfrom scipy.optimize import minimize_scalar\nimport pickle\nimport io\n#from hana_ml.algorithms.pal import trees\n#import sapdi\nimport json\n\ndef on_input(msg):\n    \n    # Obtain data\n    #df_raw = pd.read_csv(io.StringIO(msg.body), sep=\",\")\n    df =  pd.read_json(io.StringIO(msg))\n    #data =  df.values.tolist()\n    #api.send(\"output2\",data)\n    \"\"\"\n    df = pd.DataFrame()\n    df['QUOTE_ID'] = df_raw.iloc[:,0]\n    df['CUSTOMER_NAME'] = df_raw.iloc[:,1]\n    df['INDUSTRY'] = df_raw.iloc[:,2]\n    df['CREATION_DATE'] = df_raw.iloc[:,3]\n    df['COUNTRY'] = df_raw.iloc[:,4]\n    df['PRODUCT_NAME'] = df_raw.iloc[:,5]\n    df['MODEL'] = df_raw.iloc[:,6]\n    df['SUPPLY_VOLTAGE_A'] = df_raw.iloc[:,7]\n    df['COM_QUANTITY'] = df_raw.iloc[:,9]\n    df['STATUS'] = df_raw.iloc[:,8]\n    df['LISTPRICE'] = df_raw.iloc[:,10].astype(float)\n    df['MANUFACTURING_COST'] = df_raw.iloc[:,11].astype(float)\n    df['QUOTED_PRICE'] = df_raw.iloc[:,12].astype(float)    \n    \"\"\"\n    def compnent_analytics(df):\n\n        regressor = GradientBoostingRegressor()\n        X = pd.DataFrame(df, columns=['LISTPRICE','MANUFACTURING_COST']).copy()\n        y = pd.Series(df['QUOTED_PRICE']).copy()\n        regressor.fit(X, y)\n        #model_cp=pickle.dump(regressor, open(\"xgb_V2.pickle.dat\", \"wb\")) \n        #api.send(\"modelBlob\", model_cp)\n        model_cp = pickle.dumps(regressor)\n        api.send(\"modelBlob\", model_cp)\n\n        df['UtilityAdj'] = regressor.predict(X)\n        df['Discount'] = 1-(df['QUOTED_PRICE']/df['LISTPRICE'])\n        df['LIST_VALUE'] = df['LISTPRICE']/df['UtilityAdj']\n        df['GP_PCT_VALUE'] = (df['QUOTED_PRICE'] - df['MANUFACTURING_COST'])/df['UtilityAdj']\n        df['GP_PCT_PRICE'] = (df['QUOTED_PRICE'] - df['MANUFACTURING_COST'])/df['QUOTED_PRICE']\n\n        return df    \n\n#Clustering using Decision tree Regressor\n    def segmentation(X,Y,seg_min=0.0,seg_max=1.0,maxm_depth=2,min_split=50,min_leaves=20):\n        clf = tree.DecisionTreeRegressor(criterion='mse',max_depth=maxm_depth,min_samples_split=min_split,min_samples_leaf=min_leaves)\n        clf = clf.fit(X,Y)\n        seg_array = clf.tree_.threshold[(clf.tree_.children_left + clf.tree_.children_right)!=-2]\n        seg_array = np.append(seg_array,[seg_min,seg_max])\n        seg_array.sort()\n\n        n_segments = len(seg_array)-1\n        segments = np.zeros((n_segments,2))\n        for i in np.arange(len(seg_array)-1):\n            segments[i,0] = seg_array[i]\n            segments[i,1] = seg_array[i+1]\n\n        leaf_array = (clf.tree_.children_left + clf.tree_.children_right)==-2\n        num_leaves = sum(leaf_array)\n        i = 0\n        leaves = np.zeros((num_leaves,3),dtype = np.int32)\n        if clf.tree_.node_count>1:\n            print('\\n%d' % (clf.tree_.node_count))\n            node_index = np.arange(clf.tree_.node_count)\n            for k in np.arange(len(leaf_array)):\n                if leaf_array[k] == True:\n                    leaves[i,0] = k\n                    leaves[i,1] = node_index[(clf.tree_.children_left==k) | (clf.tree_.children_right==k)]\n                    if sum(clf.tree_.children_left==k)==1:\n                        leaves[i,2] = 1\n                    i = i + 1\n\n            leaf_values = np.zeros((num_leaves,1))\n            leaf_sample_count = np.zeros((num_leaves,1),dtype=np.int32)\n            leaf_threshold = clf.tree_.threshold[leaves[:,1]]\n            for j in np.arange(num_leaves):\n                leaf_sample_count[segments[:,leaves[j,2]]==leaf_threshold[j]] = clf.tree_.n_node_samples[leaves[j,0]]\n                leaf_values[segments[:,leaves[j,2]]==leaf_threshold[j]] = clf.tree_.value[leaves[j,0]]\n        else:\n            leaf_sample_count = len(X)\n            leaf_values = (Y.mean())[0]\n            num_leaves - 0\n\n        segment_full = pd.DataFrame(segments, columns=['MIN','MAX'])\n        segment_full['COUNT'] = leaf_sample_count\n        segment_full['AVG_VAL'] = leaf_values\n        segment_full['SEG_ID'] = np.arange(1,(num_leaves+1))\n        segment_full = pd.DataFrame(segment_full,columns=['SEG_ID','MIN','MAX','COUNT','AVG_VAL'])\n\n        return segments, leaf_values, leaf_sample_count, segment_full\n\n    def DeleteOdds(X, y, Residual):\n        \"\"\"\n        Delete the data points that result in the largest deviance residual\n        \"\"\"\n        Index_max = np.absolute(Residual).idxmax()\n        X = X.drop(Index_max,axis=0)\n        y = y.drop(Index_max,axis=0)\n        z = pd.concat([y,X], axis = 1)    \n\n        return z\n\n\n\n    def ComLogit(Regressor, Response, Var_Include, SampleSize, alpha, data_vol, min_score):\n        \"\"\"\n        Logistics Regression with only Var_Include as regressor\n        Regressor: Dataframe of all possible independent variables\n        Response: Dataframe of dependent variables\n        Var_Include: Array of strings indicates the variables to be included in model\n        SampleSize: A integer indicating the sample size\n        alpha: the confidience used to determine whether needs variable selection\n        data_vol: maximum fraction of data points need to be retained\n        min_score: minimum score of the data quality need to be attained for data deletion\n        \"\"\"\n        Reg_Names = Regressor.columns.tolist()\n        y = Response.copy()\n        X = Regressor[Var_Include].copy()\n        X = sm.add_constant(X, prepend = True)\n        glm_binom = sm.GLM(y, X, family = sm.families.Binomial())\n        result = glm_binom.fit()\n        b = result.params\n        pvalue = result.pvalues\n        correlation = result.cov_params()\n        #The residual deviance. Or, use response residuals? (resid_response) \n        #results should be the same\n        residual = result.resid_deviance\n        #Define res, a DataFrame containing the needed information\n        res = pd.DataFrame({}, index = ['const'] + Reg_Names)\n        res['param'] = b\n        res['pvalue'] = pvalue\n        correlation = pd.DataFrame(correlation, columns = ['const'] + Reg_Names)\n        res = pd.concat([res, correlation], axis = 1)\n        res = pd.DataFrame(res, index = ['const'] + Reg_Names)\n        #Delete outliers if:\n        #1. the result is insiginificant; \n        #2. after deletion the data quality is good enough;\n        #3. the sign on the price is correct.\n        #If the sign is wrong, all the \"win\" data points could be deleted.\n        while True in (res.pvalue > alpha).values:\n            n_in = len(y)\n            if n_in/SampleSize > data_vol:\n                z = DeleteOdds(X, y, residual)\n                y_temp = pd.DataFrame(z.Response).copy()\n                X_temp = z[Var_Include].copy()\n                if Score(y_temp, X_temp, len(y_temp)) > min_score:\n                    y = y_temp.copy()\n                    X = X_temp.copy()\n                    X = sm.add_constant(X, prepend = True)\n                    glm_binom = sm.GLM(y, X, family = sm.families.Binomial())\n                    result = glm_binom.fit()\n                    b = result.params\n                    pvalue = result.pvalues\n                    correlation = result.cov_params()\n                    residual = result.resid_deviance\n                    res = pd.DataFrame({}, index = ['const'] + Reg_Names)\n                    res['param'] = b\n                    res['pvalue'] = pvalue\n                    correlation = pd.DataFrame(correlation, columns = ['const'] + Reg_Names)\n                    res = pd.concat([res, correlation], axis = 1)\n                    res = pd.DataFrame(res, index = ['const'] + Reg_Names)\n                else:\n                    break\n            else:\n                #Assigining negative pvalue to break the loop\n                #res['pvalue'] = - pvalue\n                break\n\n\n        #If either the intercept is insiginificant or price has a wrong sign then \n        #delete the intercept term        \n        if res['pvalue']['const'] > alpha[0] or res['param']['GP_PCT_VALUE'] < 0:\n                y = Response\n                X = Regressor[Var_Include]\n                glm_binom = sm.GLM(y, X, family = sm.families.Binomial())\n                result = glm_binom.fit()\n                b = result.params\n                pvalue = result.pvalues\n                correlation = result.cov_params()\n                residual = result.resid_deviance\n                res = pd.DataFrame({}, index = ['const'] + Reg_Names)\n                res['param'] = b\n                res['pvalue'] = pvalue\n                correlation = pd.DataFrame(correlation, columns = ['const'] + Reg_Names)\n                res = pd.concat([res, correlation], axis = 1)\n                res = pd.DataFrame(res, index = ['const'] + Reg_Names)\n                while True in (res.pvalue > alpha).values:\n                    n_in = len(y)\n                    if n_in/SampleSize > data_vol:\n                        z = DeleteOdds(X, y, residual)\n                        y_temp = pd.DataFrame(z.Response).copy()\n                        X_temp = z[Var_Include].copy()\n                        if Score(y_temp, X_temp, len(y_temp)) > min_score:\n                            y = y_temp.copy()\n                            X = X_temp.copy()\n                            glm_binom = sm.GLM(y, X, family = sm.families.Binomial())\n                            result = glm_binom.fit()\n                            b = result.params\n                            pvalue = result.pvalues\n                            correlation = result.cov_params()\n                            residual = result.resid_deviance\n                            res = pd.DataFrame({}, index = ['const'] + Reg_Names)\n                            res['param'] = b\n                            res['pvalue'] = pvalue\n                            correlation = pd.DataFrame(correlation, columns = ['const'] + Reg_Names)\n                            res = pd.concat([res, correlation], axis = 1)\n                            res = pd.DataFrame(res, index = ['const'] + Reg_Names)\n                        else:\n                            break\n                    else:\n                        break\n\n        return res\n\n\n\n    def ComLogit_Diff(Regressor, Response, SampleSize, alpha, data_vol, min_score):\n        \"\"\"\n        Logistics Regression with automated variable selection allowing \n        price differentiation\n        Regressor: Dataframe of all possible independent variables\n        Response: Dataframe of dependent variables\n        SampleSize: A integer indicating the sample size\n        alpha: the confidience used to determine whether needs data/variable selection\n        data_vol: maximum fraction of data points need to be retained\n        \"\"\"  \n\n        #First try to include all the Regressors\n        Reg_Names = Regressor.columns.tolist()\n        Var_Include = Reg_Names\n        res = ComLogit(Regressor, Response, Var_Include, SampleSize, alpha, data_vol, min_score) \n        #if the result is siginificant and the price sign is correct\n        if not (True in (res.pvalue > alpha).values) and res['param']['GP_PCT_VALUE'] > 0 :\n            #simply use 'res' as the result\n            return res\n            #Move on to the variable selections with two regressors\n        else:\n            Var_Include_1 = ['GP_PCT_VALUE',  'LIST_VALUE']\n            res_1 = ComLogit(Regressor, Response, Var_Include_1, SampleSize, alpha, data_vol, min_score)         \n            Var_Include_2 = ['GP_PCT_VALUE', 'TMC_VALUE']\n            res_2 = ComLogit(Regressor, Response, Var_Include_2, SampleSize, alpha, data_vol, min_score)  \n            if not (True in (res_1.pvalue > alpha).values) and res_1['param']['GP_PCT_VALUE'] > 0 :\n                return res_1\n            else:\n                return res_2\n\n    def ComLogit_Unif(Regressor, Response, SampleSize, alpha, data_vol, min_score):\n        \"\"\"\n        Logistics Regression without price differentiation\n        Regressor: Dataframe of all possible independent variables\n        Response: Dataframe of dependent variables\n        SampleSize: A integer indicating the sample size\n        alpha: the confidience used to determine whether needs data/variable selection\n        data_vol: maximum fraction of data points need to be retained\n        \"\"\"         \n        #price being the only regressor                 \n        Var_Include = ['GP_PCT_VALUE']\n        res = ComLogit(Regressor, Response, Var_Include, SampleSize, alpha, data_vol, min_score)\n        return res\n\n\n    def Main_Regression(Regressor, Response, SampleSize, alpha0, data_vol, min_score = 3.0):\n        \"\"\"\n        Main regression function that combines variable selections, outlier deletions,\n        and logistics regression to produce results that are significant and correct.\n        Regressor: Dataframe of all possible independent variables\n        Response: Dataframe of dependent variables\n        SampleSize: A integer indicating the sample size\n        alpha0: the ideal threshold for p-values\n        data_vol: maximum fraction of data points need to be retained\n        \"\"\"\n        alpha = list(alpha0)\n        #First run the regression for differentiated price\n        res = ComLogit_Diff(Regressor, Response, SampleSize, alpha, data_vol, min_score)\n        if True in (res.pvalue > alpha).values or res['param']['GP_PCT_VALUE'] < 0:\n            alpha[1] = alpha[1] + 0.02\n            res = ComLogit_Diff(Regressor, Response, SampleSize, alpha, data_vol, min_score)\n            if True in (res.pvalue > alpha).values or res['param']['GP_PCT_VALUE'] < 0:\n                alpha[1] = alpha[1] + 0.02\n                res = ComLogit_Diff(Regressor, Response, SampleSize, alpha, data_vol, min_score)  \n        #If the result is still insignificant, move on to uniform pirce\n        if True in (res.pvalue > alpha).values or res['param']['GP_PCT_VALUE'] < 0:\n            #alpha = alpha0\n            alpha = list(alpha0)\n            res = ComLogit_Unif(Regressor, Response, SampleSize, alpha, data_vol, min_score)\n            if True in (res.pvalue > alpha).values or res['param']['GP_PCT_VALUE'] < 0:\n                alpha[1] = alpha[1] + 0.02\n                res = ComLogit_Unif(Regressor, Response, SampleSize, alpha, data_vol, min_score)\n                if True in (res.pvalue > alpha).values or res['param']['GP_PCT_VALUE'] < 0:\n                    alpha[1] = alpha[1] + 0.02\n                    res = ComLogit_Unif(Regressor, Response, SampleSize, alpha, data_vol, min_score)\n        #If the result is still insignificant, there is nothing we can do, return \n        #whatever the last result is.            \n        return res\n\n\n    def WProb_(x, input_coef, b):\n        \"\"\"\n        Given a price and parameters of the package compute the win probability\n        x: input price\n        input_coef: Series containing other input parameters of the package\n        b: Series containing the estimated parameters of the regression model\n        \"\"\"                                                                                                                                                                \n        #fill the NaN value with 0 for computation\n        b = b.fillna(0.0)  \n        listp_value = input_coef['LIST_VALUE'] - 1\n        tmc = input_coef['TMC'] \n        listp = input_coef['LISTPRICE']\n        value = listp / input_coef['LIST_VALUE']\n        tmc_value = 1 - tmc / value \n        regressor=[1.0, - (x - tmc) / value, listp_value, tmc_value]\n        z = np.exp(np.dot(regressor, b)) / ( 1 + np.exp(np.dot(regressor, b)) )\n        return z\n\n\n    def Rev_(x, input_coef, b):\n        \"\"\"\n        Given a price and parameters of the package compute the NEGATIVE revenue\n        x: input price\n        input_coef: Series containing other input parameters of the package\n        b: Series containing the estimated parameters of the regression model\n        \"\"\"     \n        tmc = input_coef['TMC']                                                                                                                                                            \n        return - (x - tmc) * WProb_(x, input_coef, b)    \n\n\n    def OptPrice(Input, b):\n        \"\"\"\n        Given the input and price sensitivity information compute optimal price\n        x: input price\n        Input: Dataframe containing all the input information\n        b: Series containing the estimated parameters of the regression model\n        \"\"\"   \n        Value = Input.LISTPRICE / Input.LIST_VALUE\n        TMC = Input.TMC\n        QuotePrice = Input.GP_PCT_VALUE * Value + Input.TMC\n        #Organizing Response variables\n        Response = Input['Win']        \n        #Creating Lists for storing results\n        WP_act = list(range(len(Response)))\n        gp_pct_act = list(range(len(Response)))\n        Discount_act = list(range(len(Response)))\n        WP_opt = list(range(len(Response)))\n        gp_pct_opt = list(range(len(Response)))\n        OptPrice = list(range(len(Response)))\n        Discount_opt = list(range(len(Response)))\n\n        for i in range(len(Response)):\n            input_coef = Input.iloc[i]  \n            x_act = QuotePrice.iloc[i]\n            c = TMC.iloc[i]  \n            p_l = Input.LISTPRICE.iloc[i]    \n            WP_act[i] = WProb_(x_act, input_coef, b)\n            gp_pct_act[i] = (x_act - c) / x_act \n            Discount_act[i] = (p_l - x_act) / p_l\n\n            res = minimize_scalar(Rev_, bounds = (c, p_l), args = (input_coef, b), method = 'bounded') \n            x_opt = res.x\n            WP_opt[i] = WProb_(x_opt, input_coef, b)\n            gp_pct_opt[i] = (x_opt - c) / x_opt\n            OptPrice[i] = x_opt\n            Discount_opt[i] = (p_l - x_opt) / p_l\n\n\n        #Combining the outcomes\n        #Add columns discount_act and discount_opt\n\n        Output = pd.DataFrame({})\n        Output['Response'] = Response.values\n        Output['QuotePrice'] = QuotePrice.values\n        Output['WP_act'] = WP_act\n        Output['gp_pct_act'] = gp_pct_act\n        Output['Discount_act'] = Discount_act\n        Output['OptPrice'] = OptPrice\n        Output['WP_opt'] = WP_opt\n        Output['gp_pct_opt'] = gp_pct_opt\n        Output['Discount_opt'] = Discount_opt\n        return Output\n\n\n    def Business_Case(Output, re_output = False):\n        \"\"\"\n        Compute the business case as a control for choosing segmentation.\n        Output: the output from OptPrice, records the optimal prices, etc\n        re_output: whether return the table for the business case result\n        Return: returns the sum of business case values and if re_ouput is specified\n        to be true, also returns the table of each business case value.\n        \"\"\"    \n        Revenue_Diff = list(range(len(Output)))\n        for i in range(len(Output)):\n            p_opt = Output.OptPrice.iloc[i]\n            p_act = Output.QuotePrice.iloc[i]\n            q_opt = Output.WP_opt.iloc[i]\n            q_act = Output.WP_act.iloc[i]\n            if Output.Response.iloc[i] == 1:\n                if p_opt > p_act:\n                    Revenue_Diff[i] = q_opt/q_act * p_opt - p_act\n                else:\n                    Revenue_Diff[i] = p_opt - p_act\n            else:\n                if p_opt > p_act:\n                    Revenue_Diff[i] = 0.0\n                else:\n                    Revenue_Diff[i] = (1 - (1 - q_opt)/(1 - q_act)) * p_opt                \n        BC_Value = np.sum(Revenue_Diff)\n        Output['Business_Case'] = Revenue_Diff\n        if re_output == False:\n            return BC_Value\n        else:\n            return BC_Value, Output\n\n\n    def unique_arr(arr):\n        \"\"\"\n        Helper function to return the unique values in a ndarray.\n        np.unique() can only deal wiht 1-D array.\n        \"\"\"\n        arr = np.asarray(arr)\n        uniques = []\n        for i in range(len(arr)):\n            if list(arr[i]) not in uniques:\n                uniques.append(list(arr[i]))\n        return uniques\n\n\n    def Extract_Input(Input):\n        \"\"\"\n        Helper function to extract from Input the columns for regression.\n        Input: a data frame contains all the columns\n        Return: Response -- response variable of the regression\n                Regressor -- Regressor for the regression\n                Samplesize -- Sample size for the regression\n        \"\"\"    \n        Response = pd.DataFrame(Input.Win).copy()\n        Response.columns = ['Response']\n        Regressor = pd.DataFrame(Input, columns = ['LIST_VALUE']).copy()\n        VALUE = Input.LISTPRICE / Input.LIST_VALUE\n        TMC_VALUE = Input.TMC / VALUE\n        Regressor['TMC_VALUE'] = 1 - TMC_VALUE\n        Regressor['LIST_VALUE'] = Regressor['LIST_VALUE'] - 1\n        Regressor['GP_PCT_VALUE'] = - Input.GP_PCT_VALUE\n        Reg_Names = ['GP_PCT_VALUE', 'LIST_VALUE', 'TMC_VALUE']\n        Regressor = pd.DataFrame(Regressor, columns = Reg_Names)\n        SampleSize = len(Response)\n        return Response, Regressor, SampleSize\n\n    def Score(Response, Regressor, SampleSize):\n        \"\"\"\n        Helper function to calculate the scores for the quality of the data input \n        for regression.\n        Response: Response variable for the regression model\n        Regressor: Predictors for the regression model\n        Return: The score value for the data inputs\n        \"\"\"   \n        Quotes_Score = 0\n        Win_Score = 0\n        Brands_Score = 1\n        Market_Score = 0\n        CV_GP_Score = 0\n        #Computing Quotes_Score\n        if SampleSize > 49:\n            Quotes_Score = Quotes_Score + 1\n            if SampleSize > 99:\n                Quotes_Score = Quotes_Score + 1\n        #Computing Win_Score\n        Win_Rates = sum(Response['Response']) / SampleSize\n        if Win_Rates > 0.04:\n            Win_Score = Win_Score + 1\n            if Win_Rates > 0.08:\n                Win_Score = Win_Score + 1\n        #Computing Market_Score\n        if 'LIST_VALUE' in Regressor.columns:\n            Market_Position = max(Regressor['LIST_VALUE']) - min(Regressor['LIST_VALUE'])\n            if Market_Position < 5:\n                Market_Score = Market_Score + 1\n                if Market_Position < 2:\n                    Market_Score = Market_Score + 1\n        else:\n            Market_Score = 1\n        #Computing CV_GP_Score\n        CV_GP = - Regressor['GP_PCT_VALUE'].std() / Regressor['GP_PCT_VALUE'].mean()    \n        if CV_GP < 0.5:\n            CV_GP_Score = CV_GP_Score + 1\n            if CV_GP < 0.25:\n                CV_GP_Score = CV_GP_Score + 1\n        #Computing the overall score\n        if Win_Score == 0:\n            score = 0\n        else:\n            score = Quotes_Score + Win_Score + Brands_Score + Market_Score + CV_GP_Score\n        return score\n    def Label_Seg(Input_Data, Infile_Data):                  \n        \"\"\"\n        Labels each transaction in the original data to the segment it belongs.\n\n        Parameters\n        ----------\n        Input_Data: A dataframe that contains all the original transaction data.\n        Input_Seg: A dataframe that contains the segmentation information for each OD cluster pair\n        f_name: The data directory and file names to write the file\n        version: The version of data and cluster level\n\n        Return\n        -------\n        The revised input data.\n\n        \"\"\"                                     \n        seg_id = list(np.zeros(len(Input_Data), dtype = 'i4'))\n        Discount_act = list(np.zeros(len(Input_Data)))\n        Discount_sd =list(np.zeros(len(Input_Data)))\n        lw = list(np.zeros(len(Input_Data)))\n        up = list(np.zeros(len(Input_Data)))\n\n\n        for i in range(len(Input_Data)):\n            brand = Input_Data.loc[i,'PRODUCT_BRAND']\n            Value = Input_Data.loc[i,'LIST_VALUE']\n\n\n\n\n            if len(Infile_Data) > 1:\n                for j in range(len(Infile_Data)):\n                    if ( (brand == Infile_Data.loc[j, 'PRODUCT_BRAND']) and\n                    (Value > Infile_Data.loc[j, 'VAL_MIN']) and (Value <= Infile_Data.loc[j, 'VAL_MAX'])   ):\n                        seg_id[i] = Infile_Data.loc[j, 'SEGMENT_ID']\n\n                        lw[i] = 0\n                        up[i]=0\n            else:\n                seg_id[i] = Infile_Data.loc[0, 'SEGMENT_ID']\n\n                lw[i] = 0\n                up[i] = 0\n        Input_Data['SEGMENT_ID'] = seg_id\n\n\n        return Input_Data      \n    def Compute_Opt_Price(Input_Data, Infile_Data):                  \n        \"\"\"\n        Compute the optimal price according to the features and the corresponding parameter estimates in\n        \"Input_Seg\" for each transaction in \"Input_Data\".\n\n        Parameters\n        ----------\n        Input_Data: A dataframe that contains all the original transaction data / new request for quotes\n        Input_Seg: A dataframe that contains the segmentation AND regression information (FINAL_REG_SEG)\n        f_name: The data directory and file names to write the file\n\n\n        Outputs:\n        --------\n        Writes the labeled data to a new file.\n\n        Return\n        -------\n        The revised input data.\n\n        \"\"\"    \n        opt_price = list(np.zeros(len(Input_Data)))\n        x_opt = list(np.zeros(len(Input_Data))) \n        WP_act = list(np.zeros(len(Input_Data)))\n        WP_opt = list(np.zeros(len(Input_Data)))\n\n\n\n        for i in np.arange(len(Input_Data)):\n            if i % 1000 == 0:\n                print ('Processing quotes.')\n\n            seg_id = Input_Data.loc[i, 'SEGMENT_ID']\n            k = Input_Data.loc[i, 'TMC']\n            l = Input_Data.loc[i, 'LISTPRICE']\n            param = Infile_Data.loc[seg_id, ['VAL_const', 'VAL_GP_PCT_VALUE', 'VAL_LIST_VALUE','VAL_TMC_VALUE']]\n            param = param.fillna(0.0)\n            input_coef = Input_Data.iloc[i]\n\n            res1 = minimize_scalar( Rev_, bounds = (k,l), args = (input_coef, param), method = 'bounded' )\n\n\n            opt_price[i] = res1.x\n            x_opt[i] = opt_price[i]\n            x_act = Input_Data.loc[i, 'QUOTED_PRICE']\n            WP_act[i] = WProb_(x_act, input_coef, param)\n            WP_opt[i] = WProb_(x_opt[i], input_coef, param)\n\n\n\n        Input_Data['OPT_PRICE'] = opt_price\n        Input_Data['WIN_ACT'] = WP_act\n        Input_Data['WIN_OPT'] = WP_opt\n\n        return Input_Data\n\n   \n    agg_df_final = compnent_analytics(df)\n    agg_df_final['PRODUCT_BRAND'] = agg_df_final['MODEL']\n    max_depth = 2\n    min_split = 50\n    min_leaf = 20\n\n    brand_details = []\n    group_brand = agg_df_final.groupby('PRODUCT_BRAND')\n    j = 1\n    for brand, grp in group_brand:\n        brand_detail = pd.DataFrame({'PRODUCT_BRAND':[brand],'COUNT':[len(grp)],'AVG_VAL':[grp['GP_PCT_PRICE'].mean()],'LEAD_BRAND_ID':[j]},columns=['LEAD_BRAND_ID','PRODUCT_BRAND','COUNT','AVG_VAL'])\n        brand_details.append(brand_detail)\n        j = j+1\n    \n    brand_segment = pd.concat(brand_details, ignore_index=True)\n    brand_segment = pd.DataFrame(brand_segment,columns=['LEAD_BRAND_ID','PRODUCT_BRAND','COUNT','AVG_VAL']).copy()\n    lead_seg_sub = []\n\n    for i in np.arange(len(brand_segment)):\n        lead_seg_id = brand_segment.loc[i,'LEAD_BRAND_ID']\n        leading_brand = brand_segment.loc[i,'PRODUCT_BRAND']\n        val_seg_input = pd.DataFrame(agg_df_final[agg_df_final.PRODUCT_BRAND==leading_brand].copy(), columns=['LIST_VALUE','GP_PCT_VALUE'])\n        val_seg_input.columns = ['PREDICTOR','RESPONSE']\n        val_segments, val_leaf_values, val_leaf_sample_count, val_seg_out_temp = segmentation(val_seg_input[['PREDICTOR']],val_seg_input[['RESPONSE']], 0.0, 10000.0, max_depth, min_split, min_leaf)\n        val_seg_out_temp.columns = ['VAL_SEG_ID','VAL_MIN','VAL_MAX','VAL_COUNT','VAL_AVG_VAL']           \n        val_seg_out_temp['LEAD_BRAND_ID'] = lead_seg_id\n        lead_seg_sub.append(val_seg_out_temp)\n\n    val_seg_out = pd.concat(lead_seg_sub, ignore_index=True)\n    val_seg_out = pd.DataFrame(val_seg_out, columns=['LEAD_BRAND_ID','VAL_SEG_ID','VAL_MIN','VAL_MAX','VAL_COUNT','VAL_AVG_VAL'])\n    full_io_table_out = pd.merge(brand_segment,val_seg_out,left_on='LEAD_BRAND_ID',right_on='LEAD_BRAND_ID',how='inner')\n    Input_Data = agg_df_final\n    Input_Seg = full_io_table_out\n    Input_Data.index=np.arange(len(Input_Data))\n    #selecting cluster having more than 100 data points \n    Input_Seg=Input_Seg[Input_Seg['COUNT']> 100]\n    Input_Seg.index=np.arange(len(Input_Seg))\n    #Ideal threshold for p-values\n    alpha0 = [0.15, 0.01, 0.15, 0.15]\n    #Threshold for minimum percentage of data to keep when deleting outliers\n    data_vol = 0.98\n    #@huz_0617: Adding another control variable -- min_score to guarantee that\n    #the quality of the segmented data are good enough\n    min_score = 3.0\n    #Minimum number of data to be retained in the segments\n    min_data = 50.0\n    #Add another control variales: average discounts level as determining whether \n    #to further segment or not.\n    #Compare both the unsegmented and segmented average discounts level, say PWR_Discount and HW_Discount with the\n    #actual average discounts level/confidence interval: [Avg_Discount - beta*sigma, Avg_Discount + beta*sigma]. \n    #If both discounts lie in the interval, then compare business case. If one lie in, the other one lie out, choose\n    #the one lie in. If both lie out of the interval, choose the one closer to the interval.  \n    #Controling the interval of the acceptable price range\n    beta = 1.0\n    #beta = 0.5 #Cannot result in further segmentation\n\n    Input_Data['TMC']= Input_Data['MANUFACTURING_COST']\n    Input_Data['Win']= np.where(Input_Data['STATUS'] == 'Y',1,0)\n\n    LEAD_Col_Names = np.asarray(['BC_Value', 'const', 'GP_PCT_VALUE', 'LIST_VALUE', 'TMC_VALUE'])\n    LEAD_Col_Names = LEAD_Col_Names.astype(np.object)\n    LEAD_Col_Names = ['LEAD_'] + LEAD_Col_Names\n    LEAD_Col_Names = np.append(['PRODUCT_BRAND'], LEAD_Col_Names)\n    LEAD_out = pd.DataFrame(columns = LEAD_Col_Names )\n    VAL_Col_Names = np.asarray(['MIN', 'BC_Value', 'Discount_act', 'Discount_opt', 'const', 'GP_PCT_VALUE', 'LIST_VALUE', 'TMC_VALUE'])\n    VAL_Col_Names = VAL_Col_Names.astype(np.object)\n    VAL_Col_Names = ['VAL_'] + VAL_Col_Names\n    VAL_Col_Names = np.append(['PRODUCT_BRAND'], VAL_Col_Names)\n    VAL_out = DataFrame(columns = VAL_Col_Names)\n    for brand in Input_Seg['PRODUCT_BRAND'].unique():\n        #The segmented part of the segmentation information\n        LEAD_Seg = Input_Seg[Input_Seg['PRODUCT_BRAND'] == brand]\n        #The data that satisfies the segmentation\n        LEAD_Seg_Data = pd.DataFrame(Input_Data[Input_Data['PRODUCT_BRAND'] == brand]).copy()\n        LEAD_Seg_Data.index = range(len(LEAD_Seg_Data))\n        Response, Regressor, SampleSize = Extract_Input(LEAD_Seg_Data)\n        LEAD_Score = Score(Response, Regressor, SampleSize)\n        reg_res =  Main_Regression(Regressor, Response, SampleSize, alpha0, data_vol)\n        LEAD_param = reg_res.param\n        Output = OptPrice(LEAD_Seg_Data, LEAD_param)\n\n        LEAD_BC_Value = Business_Case(Output) \n        temp = np.concatenate([[brand, LEAD_BC_Value], LEAD_param])\n        temp = temp.reshape((1, len(temp)))\n        temp = pd.DataFrame(temp, columns = LEAD_Col_Names)\n        LEAD_out = pd.concat([LEAD_out, temp])\n        for val_min, val_max in unique_arr(LEAD_Seg[['VAL_MIN', 'VAL_MAX']]):\n            #The segmented part of the segmentatoin information\n            VAL_Seg = LEAD_Seg[LEAD_Seg['VAL_MIN'] == val_min]\n            #The data that satisfies the segmentation\n            VAL_Seg_Data = pd.DataFrame(LEAD_Seg_Data[(LEAD_Seg_Data.LIST_VALUE >= val_min) & (LEAD_Seg_Data.LIST_VALUE <= val_max)]).copy()\n            VAL_Seg_Data.index = range(len(VAL_Seg_Data))\n            Response, Regressor, SampleSize = Extract_Input(VAL_Seg_Data)\n            VAL_Score = Score(Response, Regressor, SampleSize)\n            #The default business case value and parameter estimates are set\n            #to the case without further segmentation unless the following \n            #if statements are satisfied.\n            VAL_param = LEAD_param\n            Output_NoSeg = OptPrice(VAL_Seg_Data, LEAD_param)\n\n            VAL_BC_Value = Business_Case(Output_NoSeg)\n            Discount_NoSeg_Avg = Output_NoSeg['Discount_opt'].mean()\n            Discount_act_Avg = Output_NoSeg['Discount_act'].mean()\n            Discount_act_sd = Output_NoSeg['Discount_act'].std()\n            Discount_opt_Avg = Discount_NoSeg_Avg\n            #If there is enough sample for further segmentation\n            if SampleSize >= min_data and Score(Response, Regressor, SampleSize) >= min_score:\n                try:\n                    reg_res = Main_Regression(Regressor, Response, SampleSize, alpha0, data_vol)\n                except:\n                    bad_data = VAL_Seg_Data\n                #If the segmented model returns the right prediction\n                if reg_res['param']['GP_PCT_VALUE'] > 0:\n                    Output_Seg = OptPrice(VAL_Seg_Data, reg_res.param)\n\n                    BC_Value_Seg = Business_Case(Output_Seg)\n                    Discount_Seg_Avg = Output_Seg['Discount_opt'].mean()\n                    #If both the average discounts of segment and unsegmented model lie in the acceptable region\n                    if ((Discount_NoSeg_Avg >= Discount_act_Avg - beta*Discount_act_sd) and \n                    (Discount_NoSeg_Avg <= Discount_act_Avg + beta*Discount_act_sd) and\n                    (Discount_Seg_Avg >= Discount_act_Avg - beta*Discount_act_sd) and\n                    (Discount_Seg_Avg <= Discount_act_Avg + beta*Discount_act_sd)):\n                        #If the segmented model has a higher business case value\n                        if BC_Value_Seg > VAL_BC_Value:\n                            VAL_BC_Value = BC_Value_Seg\n                            VAL_param = reg_res.param\n                            Discount_opt_Avg = Discount_Seg_Avg\n                            FIN_Score = VAL_Score\n                            Level = 'VAL'\n                    #Else if the segment model produces a closer average discount to the actual discount level        \n                    elif np.absolute(Discount_Seg_Avg - Discount_act_Avg) < np.absolute(Discount_NoSeg_Avg - Discount_act_Avg):\n                        VAL_BC_Value = BC_Value_Seg\n                        VAL_param = reg_res.param\n                        Discount_opt_Avg = Discount_Seg_Avg\n                        FIN_Score = VAL_Score\n                        Level = 'VAL'\n                    else:\n                        print ('\\n Discount level is far from actual level!')\n                else:\n                    print ('\\n Wrong prediction in the segmented model!')\n            else:\n                print ('\\n Not enough data or bad data in the segmented model!')\n            temp = np.concatenate([[brand, val_min, VAL_BC_Value, Discount_act_Avg, Discount_opt_Avg], VAL_param])\n            temp = temp.reshape((1, len(temp)))\n            temp = pd.DataFrame(temp, columns = VAL_Col_Names)\n            VAL_out = pd.concat([VAL_out, temp]) \n\n    #Full output: Output_Seg\n\n    Input_Seg['VAL_MIN']=Input_Seg['VAL_MIN'].astype(np.float)\n    VAL_out['VAL_MIN']= VAL_out['VAL_MIN'].astype(np.float)\n    Output_Seg = pd.merge(Input_Seg, LEAD_out, on = 'PRODUCT_BRAND').reset_index()\n    Output_Seg = pd.merge(Output_Seg, VAL_out, on = ['PRODUCT_BRAND', 'VAL_MIN']).reset_index()\n    Output_Seg['SEGMENT_ID'] = np.arange(len(Output_Seg))\n    cols=Output_Seg.columns.drop('PRODUCT_BRAND')\n    Output_Seg[cols]=Output_Seg[cols].apply(pd.to_numeric, errors='coerce')\n    Input_Data1 = Label_Seg(Input_Data, Output_Seg)\n    Input_Data2 = Compute_Opt_Price(Input_Data, Output_Seg)\n    discount_data = pd.DataFrame(Input_Data2,columns=['PRODUCT_BRAND','Discount']).copy()\n    discount_data['Discount_mean']=discount_data['Discount']\n    discount_data['Discount_std']=discount_data['Discount']\n    discount_agg_model = discount_data.groupby('PRODUCT_BRAND').agg({'Discount_mean':np.mean,'Discount_std':np.std}).reset_index()\n    del discount_data['Discount_mean']\n    del discount_data['Discount_std']\n    final_data = pd.merge(Input_Data2,discount_agg_model,on='PRODUCT_BRAND',how='left')\n    final_data['key1'] = 0\n    final_data['key2'] = 1\n    final_data['low']= 1-final_data['Discount_mean']- 2*final_data['Discount_std']\n    final_data['min_level'] = (final_data[['key2','low']]).min(axis=1)\n    final_data['max_min_level']=final_data[['min_level','key1']].max(axis=1)\n    final_data['low_bound'] = final_data['max_min_level']*final_data['LISTPRICE']\n    final_data['up']= 1-final_data['Discount_mean']+2*final_data['Discount_std']\n    final_data['min_level_up'] = (final_data[['key2','up']]).min(axis=1)\n    final_data['max_min_level_up']=final_data[['min_level_up','key1']].max(axis=1)\n    final_data['up_bound'] = final_data['max_min_level_up']*final_data['LISTPRICE']\n    final_data['low_bound'] =np.where(final_data['low_bound']<final_data['TMC'],final_data['TMC'],final_data['low_bound'])\n    final_data['up_bound'] =np.where(final_data['up_bound']<final_data['OPT_PRICE'],final_data['LISTPRICE'],final_data['up_bound'])\n    \n    fin_opt_price = pd.DataFrame({ 'QUOTE_ID': final_data['QUOTE_ID'] ,'REFRESH_MONTH': 'SEP', 'CUSTOMER_NAME': final_data['CUSTOMER_NAME'],\n                              'INDUSTRY': final_data['INDUSTRY'], 'CREATION_DATE': final_data['CREATION_DATE'],\n                              'COUNTRY': final_data['COUNTRY'], 'PRODUCT_NAME': final_data['MODEL'], 'QUANTITY': final_data['COM_QUANTITY'],\n                              'LISTPRICE': final_data['LISTPRICE'],'MANUFACTURING_COST': final_data['MANUFACTURING_COST'],\n                              'QUOTED_PRICE': final_data['QUOTED_PRICE'],'OPTIMUM_PRICE': final_data['OPT_PRICE'],\n                              'PROB_WIN_OPTIMAL':final_data['WIN_OPT'],'OPTIMAL_LOWER':final_data['low_bound'],'OPTIMAL_HIGHER':final_data['up_bound'],\n                              'PROB_WIN_ACTUAL': final_data['WIN_ACT']})    \n    fin_opt_price.fillna(0, inplace=True)\n    fin_opt_price = fin_opt_price.round(6)\n    \n\n    fin_Output_Seg = pd.DataFrame({ 'SEGMENT_ID': Output_Seg['SEGMENT_ID'] ,'LEAD_BRAND_ID': Output_Seg['LEAD_BRAND_ID'], \n                                'LEADING_PRODUCT': Output_Seg['PRODUCT_BRAND'],'COUNT': Output_Seg['COUNT'],\n                              'VAL_SEG_ID': Output_Seg['VAL_SEG_ID'], 'VAL_MIN': Output_Seg['VAL_MIN'],\n                              'VAL_MAX': Output_Seg['VAL_MAX'], 'VAL_COUNT': Output_Seg['VAL_COUNT'], \n                               'LEAD_CONST': Output_Seg['LEAD_const'], 'LEAD_GP_PCT_VALUE': Output_Seg['LEAD_GP_PCT_VALUE'],\n                              'LEAD_LIST_VALUE': Output_Seg['LEAD_LIST_VALUE'],'LEAD_TMC_VALUE': Output_Seg['LEAD_TMC_VALUE'],\n                              'VAL_CONST': Output_Seg['VAL_const'],'VAL_GP_PCT_VALUE': Output_Seg['VAL_GP_PCT_VALUE'],\n                              'VAL_LIST_VALUE':Output_Seg['VAL_LIST_VALUE'],'VAL_TMC_VALUE':Output_Seg['VAL_TMC_VALUE']})  \n    fin_Output_Seg.fillna(0, inplace=True)\n    fin_Output_Seg = fin_Output_Seg.round(6)\n\n\n\n\n#fin_opt_price write-back to Hana Output Table \"TA_IMC_CPQ_TRAINING_OPTIMAL_PRICE\"\n#fin_Output_Seg write-back to Hana Output Table \"TA_IMC_CPQ_TRAINING_SEGMENTS\"\n\n\n    \n    #api.send(\"output\",'success')\n    #data = [fin_opt_price.columns.values.tolist()] + fin_opt_price.values.tolist()\n    #data=fin_opt_price.dtypes\n    #data=list(final_data['LISTPRICE'])\n    #data = [df.columns.values.tolist()] + df.values.tolist()\n    #@api.send(\"output\",data)\n    \n    #data =  fin_opt_price.values.tolist()\n    #data2 = [fin_Output_Seg.columns.values.tolist()] + Output_Seg.values.tolist()\n    data2 =  fin_opt_price.values.tolist()\n    #api.send(\"output2\",'success')\n    #api.send(\"output2\",data2)\n    #f = '{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}'  # format final_data\n    f2 = '{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}' # format Output_Seg\n    #for i in data:\n        #api.send(\"output1\",f.format(*i)+'\\n')\n        #print(\"\")\n    \n    \n    for j in data2:\n        api.send(\"output2\",f2.format(*j)+'\\n')\n    \n\n#final_data write-back to Hana Output Table \"TA_IMC_CPQ_TRAINING_OPTIMAL_PRICE\"\n#Output_Seg write-back to Hana Output Table \"TA_IMC_CPQ_TRAINING_SEGMENTS\"\n\n\n\n\n\n       \n\n\napi.set_port_callback(\"input2\", on_input)"
				},
				"additionalinports": [
					{
						"name": "input2",
						"type": "string"
					}
				],
				"additionaloutports": [
					{
						"name": "output2",
						"type": "message"
					},
					{
						"name": "modelBlob",
						"type": "blob"
					}
				]
			},
			"name": "python3operator111"
		},
		"python3operator21": {
			"component": "com.sap.system.python3Operator",
			"metadata": {
				"label": "Operators Complete",
				"x": 1197.9999923706055,
				"y": 72,
				"height": 80,
				"width": 120,
				"extensible": true,
				"config": {
					"script": "# When both input ports signals arive, the Artifact Producer & Submit Metrics have completed - safe to terminate the graph.\n\ndef on_inputs_ready( artifact_id):\n    # both input ports have data - previous operators have completed. Send a message as output to stop the graph\n    api.send(\"output\", api.Message(\"Operators complete.\"))\n\napi.set_port_callback(\"artifactId\", on_inputs_ready)\n\n"
				},
				"additionalinports": [
					{
						"name": "metricsResponse",
						"type": "message"
					},
					{
						"name": "artifactId",
						"type": "string"
					}
				],
				"additionaloutports": [
					{
						"name": "output",
						"type": "message"
					}
				]
			},
			"name": "python3operator2"
		},
		"tostringconverter21": {
			"component": "com.sap.util.toStringConverter",
			"metadata": {
				"label": "ToString Converter",
				"x": 1082.9999933242798,
				"y": 87,
				"height": 50,
				"width": 50,
				"config": {}
			},
			"name": "tostringconverter2"
		},
		"tomessageconverter11": {
			"component": "com.sap.util.toMessageConverter",
			"metadata": {
				"label": "ToMessage Converter",
				"x": 967.9999942779541,
				"y": 87,
				"height": 50,
				"width": 50,
				"config": {}
			},
			"name": "tomessageconverter1"
		},
		"artifactproducer11": {
			"component": "com.sap.ml.artifact.producer",
			"metadata": {
				"label": "Artifact Producer",
				"x": 782.9999952316284,
				"y": 132,
				"height": 80,
				"width": 120,
				"extensible": true,
				"config": {
					"artifactName": "XGB_v2",
					"artifactKind": "model",
					"versionControl": "conf",
					"script": "package main\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"os\"\n\t\"path\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"../../../vsystemclient\"\n)\n\nvar (\n\tgBearerToken   string\n\tgDatahubUser   string\n\tgDatahubTenant string\n\tgConnectionID  = \"DI_DATA_LAKE\"\n\tgEndpoint      string\n\tgTimeout       = 3 * time.Second\n)\n\nvar (\n\tflagUsingvSystem = true\n\tflagUsingTLSCA   = true\n)\n\nvar ( //deprecated\n\tflagCheckRuntimeStorage = true\n\tflagUsingFileStorage    = true\n\tgRuntimeStoragePath     = \"/tenant/runtime\"\n)\n\nvar Log func(string)\nvar Logf func(string, ...interface{})\nvar Errorf func(string, ...interface{})\n\nvar OutArtifactId func(interface{})\nvar OutArtifact func(interface{})\nvar OutError func(interface{})\n\nvar GetGraphHandle func() string\nvar GetString func(string) string\n\nvar internalClient *http.Client\n\ntype ArtifactPostRequestMetaData struct {\n\tName        string `json:\"name\"`\n\tKind        string `json:\"kind\"`\n\tURI         string `json:\"uri\"`\n\tDescription string `json:\"description,omitempty\"`\n\tType        string `json:\"type\"`\n\tExecutionID string `json:\"executionId\"`\n}\n\nvar once sync.Once\nvar lock = &sync.Mutex{}\nvar counter *uint64\n\nfunc GetSingletonCounter() uint64 {\n\tonce.Do(func() {\n\t\tcounter = new(uint64)\n\t})\n\n\tlock.Lock()\n\tdefer lock.Unlock()\n\n\tactiveCounter := *counter\n\t*counter++\n\treturn activeCounter\n}\n\nfunc InPath(val interface{}) {\n\n\tpath, ok := val.(string)\n\tif !ok {\n\t\tProcessErrorInArtifact(\"Path\", errors.New(\"\"))\n\t\treturn\n\t}\n\n\tLog(\"ArtifactProducer: creating metadata for artifact production\")\n\tmetaData, artifactConf, operatorConf, err := createExistingArtifactRequestMetaDataNew(GetString, GetGraphHandle(), path)\n\tif nil != err {\n\t\tProcessErrorInArtifact(\"CreateMetadata\", err)\n\t\treturn\n\t}\n\n\tLogf(\"ArtifactProducer: checking path on SDL\")\n\tmetaDataURI, _, err := ProduceArtifactInMode(executeCheckingOfFileStatus, []byte{}, *metaData, artifactConf.endpoint, operatorConf.service)\n\tif nil != err {\n\t\tProcessErrorInArtifact(\"ProduceArtifact\", err)\n\t\treturn\n\t}\n\n\tLogf(\"ArtifactProducer: registering artifact to the following endpoint: %q\", artifactConf.endpoint)\n\t_, responseMetadata, err := registerArtifact(metaData, *metaDataURI, artifactConf.endpoint)\n\tif nil != err {\n\t\tProcessErrorInArtifact(\"RegisterArtifact\", err)\n\t\treturn\n\t}\n\n\tmessage := CreateMessage(responseMetadata.ID, metaData.Name, metaData.Kind)\n\tvsystemclient.TrySendingToPort(OutArtifact, message, gOperatorConfig.OperatorName, Logf)\n}\n\nfunc InArtifact(val interface{}) {\n\tartifact, ok := val.([]byte)\n\tif !ok {\n\t\tProcessErrorInArtifact(\"Conversion\", errors.New(\"message body cannot be converted to []byte\"))\n\t\treturn\n\t}\n\tLogf(\"ArtifactProducer: called InArtifact with length: %q\", len(artifact))\n\n\tLog(\"ArtifactProducer: creating metadata for artifact production\")\n\tmetaData, artifactConf, operatorConf, err := createNewArtifactRequestMetaDataNew(GetString, GetGraphHandle(), GetSingletonCounter())\n\tif nil != err {\n\t\tProcessErrorInArtifact(\"CreateMetadata\", err)\n\t\treturn\n\t}\n\n\tLogf(\"ArtifactProducer: producing file on SDL\")\n\tmetaDataURI, _, err := ProduceArtifactInMode(executeCreatingFileOnSDLNew, artifact, *metaData, artifactConf.endpoint, operatorConf.service)\n\tif nil != err {\n\t\tProcessErrorInArtifact(\"ProduceArtifact\", err)\n\t\treturn\n\t}\n\n\tLogf(\"ArtifactProducer: registering artifact to the following endpoint: %q\", artifactConf.endpoint)\n\t_, responseMetadata, err := registerArtifact(metaData, *metaDataURI, artifactConf.endpoint)\n\tif nil != err {\n\t\tProcessErrorInArtifact(\"RegisterArtifact\", err)\n\t\treturn\n\t}\n\n\tmessage := CreateMessage(responseMetadata.ID, metaData.Name, metaData.Kind)\n\tvsystemclient.TrySendingToPort(OutArtifact, message, gOperatorConfig.OperatorName, Logf)\n}\n\nfunc CreateMessage(artifactID string, artifactName string, artifactKind string) interface{} {\n\theaders := make(map[string]interface{}, 3)\n\theaders[\"artifactID\"] = artifactID\n\theaders[\"name\"] = artifactName\n\theaders[\"kind\"] = artifactKind\n\n\tmessage := make(map[string]interface{}, 2)\n\tmessage[\"Attributes\"] = headers\n\tmessage[\"Body\"] = \"\"\n\treturn message\n}\n\nfunc Setup() {\n\tclient, err := createInternalClient()\n\tif nil != err {\n\t\tProcessErrorSetup(\"createInternalClient\", err)\n\t\treturn\n\t}\n\n\tinternalClient = client\n\n\tif flagUsingvSystem {\n\t\tvName := os.Getenv(\"VSYSTEM_APP_ID\")\n\t\tvPass := os.Getenv(\"VSYSTEM_SECRET\")\n\t\tgDatahubUser = os.Getenv(\"VSYSTEM_USER\")\n\t\tgDatahubTenant = os.Getenv(\"VSYSTEM_TENANT\")\n\t\tif len(gEndpoint) == 0 {\n\t\t\tgEndpoint = \"http://vsystem-internal:8796\"\n\t\t}\n\t\ttoken, err := vsystemclient.Auth(vName, vPass)\n\t\tif nil != err {\n\t\t\tProcessErrorSetup(\"Auth\", err)\n\t\t\treturn\n\t\t}\n\n\t\tgBearerToken = *token\n\t\tLogf(\"ArtifactProducer: using the following endpoint with BearerToken: %q\", gEndpoint)\n\t}\n\tif flagCheckRuntimeStorage && flagUsingFileStorage {\n\t\truntimeStoragePath, ok := os.LookupEnv(\"VSYSTEM_RUNTIME_STORE_MOUNT_PATH\")\n\t\tif !ok {\n\t\t\tProcessErrorSetup(\"runtimeStorage\", errors.New(\"runtimeStorage via $VSYSTEM_RUNTIME_STORE_MOUNT_PATH not found\"))\n\t\t\treturn\n\t\t}\n\t\tif _, err := os.Stat(runtimeStoragePath); os.IsNotExist(err) {\n\t\t\tProcessErrorSetup(\"runtimeStoragePath\", err)\n\t\t\treturn\n\t\t}\n\t\tLogf(\"ArtifactProducer: using the following runtimeStorage: %q\", runtimeStoragePath)\n\t}\n}\n\n///////////////////////\n\ntype MLAPIMessage struct {\n\tID      string\n\tName    string\n\tStatus  string\n\tURI     string\n\tMessage string\n}\n\nfunc GetAPIArtifactRequest(httpMethod string, expectedStatusCode http.ConnState, endpoint string, bearerToken string, reader io.Reader) (result *MLAPIMessage, jsonResponse *[]byte, err error) {\n\n\treq, err := http.NewRequest(httpMethod, endpoint, reader)\n\tif nil != err {\n\t\treturn nil, nil, err\n\t}\n\n\treq.Header.Set(\"Authorization\", bearerToken)\n\treq.Header.Set(\"Content-Type\", \"application/json\")\n\treq.Header.Set(\"Accept\", \"application/json\")\n\n\tctx, cncl := context.WithTimeout(context.Background(), gTimeout)\n\tdefer cncl()\n\tresp, err := http.DefaultClient.Do(req.WithContext(ctx))\n\tif nil != err {\n\t\treturn nil, nil, err\n\t}\n\tdefer resp.Body.Close()\n\n\tif resp.StatusCode != int(expectedStatusCode) {\n\t\tvar addErrorMsg string\n\t\tif resp.StatusCode == http.StatusBadRequest {\n\t\t\taddErrorMsg = \"\\n\\tMake sure that the pipeline is registered in the ML Scenario Manager\"\n\t\t}\n\t\treturn nil, nil, errors.New(\"Error: \" + resp.Status + addErrorMsg)\n\t}\n\n\tif resp.StatusCode == http.StatusNoContent {\n\t\treturn nil, nil, nil\n\t}\n\n\tjsonBody, err := ioutil.ReadAll(resp.Body)\n\tif nil != err {\n\t\treturn nil, nil, err\n\t}\n\n\tif len(jsonBody) == 0 {\n\t\treturn nil, nil, errors.New(\"JSON response should not be empty\")\n\t}\n\n\tvar tmp MLAPIMessage\n\tif err := json.Unmarshal(jsonBody, &tmp); nil != err {\n\t\treturn nil, nil, err\n\t}\n\n\treturn &tmp, &jsonBody, nil\n}\n\nfunc GetPath(uri string) (path string, err error) {\n\t// assumption: uri= <protocol>:/<path>\n\tif strings.Contains(uri, \"://\") {\n\t\turiPart := strings.Split(uri, \"://\")\n\t\tif !strings.Contains(uriPart[0], \"file\") {\n\t\t\treturn \"\", fmt.Errorf(\"unspecified protocol %q\", uriPart[0])\n\t\t}\n\t\treturn uriPart[1], nil\n\t}\n\treturn uri, nil\n}\n\nfunc CreateFile(filePath string, content []byte) (rootPath string, err error) {\n\tconst pathStats os.FileMode = 0755\n\tconst fileStats os.FileMode = 0644\n\n\trootPath, _ = path.Split(filePath)\n\tif _, err := os.Stat(rootPath); os.IsNotExist(err) {\n\t\tif err = os.MkdirAll(rootPath, pathStats); err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t}\n\tif err = ioutil.WriteFile(filePath, content, fileStats); nil != err {\n\t\treturn \"\", err\n\t}\n\treturn rootPath, nil\n}\n\nfunc ProduceArtifactInMode(modeFunction func(string, string) (*requestSDLInfos, error), val []byte, metaData ArtifactPostRequestMetaData, artifactsEndpoint string, confService string) (metaDataURI *string, result *MLAPIMessage, err error) {\n\tLogf(\"ArtifactProducer: starting producer with service %q and runtimeID %q\", confService, GetGraphHandle())\n\tLogf(\"ArtifactProducer: incoming blob with length: %q\", len(val))\n\n\tartifactURI := metaData.URI\n\n\tswitch confService {\n\tcase \"SDL\":\n\t\tconnectionManagerURL := vsystemclient.CreateConnectionEndpoint(gEndpoint)\n\t\tDataLakeEndpoint := vsystemclient.CreateDataLakeEndpoint(connectionManagerURL, gConnectionID)\n\t\tdatalakeURL := createDataLakePath(artifactURI)\n\t\tLogf(\"ArtifactProducer: created datalakeURL: %q\", datalakeURL)\n\n\t\tcontentData, _, err := vsystemclient.ReceiveDatalakeInformation(DataLakeEndpoint, gBearerToken, gDatahubTenant, gDatahubUser)\n\t\tif nil != err {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\tdataLakeCon := contentData.DLData\n\n\t\tcon := createDataLakeConnectionEndpoint(dataLakeCon)\n\t\tLogf(\"ArtifactProducer: connection: %v, using CA: %v\", con, flagUsingTLSCA)\n\t\trequestSDL, err := modeFunction(con.internalCon, datalakeURL)\n\t\tif nil != err {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\trequestSDL.bearerToken = fmt.Sprintf(\"Bearer %v\", dataLakeCon.AuthToken)\n\t\trequestSDL.payload = bytes.NewReader(val)\n\n\t\tLogf(\"ArtifactProducer: hdfs command: %v\", requestSDL.endpoint)\n\t\terr = executeSDLRequestWithHDFS(requestSDL)\n\t\tif nil != err {\n\t\t\treturn nil, nil, err\n\t\t}\n\n\t\tartifactURI = createArtifactFullURI(gConnectionID, datalakeURL)\n\n\tcase \"File\":\n\t\tif !flagUsingFileStorage {\n\t\t\treturn nil, nil, fmt.Errorf(\"service %q is not supported anymore\", confService)\n\t\t}\n\n\t\tfileURI := fmt.Sprintf(\"file://%v/%v\", gRuntimeStoragePath, artifactURI)\n\t\tfilePath, err := GetPath(fileURI)\n\t\tif nil != err {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\tif _, err := CreateFile(filePath, val); err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\tLogf(\"ArtifactProducer: Artifact written to %q\", filePath)\n\t\tartifactURI = fileURI\n\n\tdefault:\n\t\treturn nil, nil, fmt.Errorf(\"ArtifactProducer: service %q is not specified, stopping function\", confService)\n\t}\n\n\tLogf(\"ArtifactProducer: URI created %q\", artifactURI)\n\treturn &artifactURI, nil, nil\n}\n\nfunc registerArtifact(metaData *ArtifactPostRequestMetaData, URI string, artifactsEndpoint string) (*string, *MLAPIMessage, error) {\n\n\tmetaData.URI = URI\n\tvar buf bytes.Buffer\n\tif err := json.NewEncoder(&buf).Encode(&metaData); err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tLogf(\"ArtifactProducer: writing to endpoint %q\", artifactsEndpoint)\n\tLogf(\"ArtifactProducer: using request json %q\", &buf)\n\tm, jsonBody, err := GetAPIArtifactRequest(http.MethodPost, 201, artifactsEndpoint, gBearerToken, &buf)\n\tif nil != err {\n\t\treturn nil, nil, fmt.Errorf(\"PostApiArtifact: %v\", err)\n\t}\n\n\tLogf(\"ArtifactProducer: response received : %q\", *jsonBody)\n\tLogf(\"ArtifactProducer: artifact registered with ID %q\", m.ID)\n\treturn &m.ID, m, nil\n}\n\nfunc executeCreatingFileOnSDLNew(connection string, datalakeURL string) (*requestSDLInfos, error) {\n\tfileURLPath := createURIForSDL(connection, datalakeURL)\n\tcreateFileURL := createURIForCreatingFileOnSDL(fileURLPath)\n\treturn &requestSDLInfos{\n\t\thttpMethod:         http.MethodPut,\n\t\tendpoint:           createFileURL,\n\t\tcontentType:        \"application/octet-stream\",\n\t\texpectedStatusCode: http.StatusCreated,\n\t}, nil\n}\n\nfunc executeCheckingOfFileStatus(connection string, datalakeURL string) (*requestSDLInfos, error) {\n\tu, err := url.Parse(connection)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"connection URL parse error: %v\", err)\n\t}\n\tu.Path = path.Join(u.Path, \"webhdfs\", \"v1\", datalakeURL)\n\n\tfileURLPath := u.String()\n\tgetFileStatusURL := fmt.Sprintf(\"%v?op=GETFILESTATUS\", fileURLPath)\n\treturn &requestSDLInfos{\n\t\thttpMethod:         http.MethodGet,\n\t\tendpoint:           getFileStatusURL,\n\t\tcontentType:        \"application/json\",\n\t\texpectedStatusCode: http.StatusOK,\n\t}, nil\n}\n\nfunc createInternalClient() (*http.Client, error) {\n\tif flagUsingTLSCA {\n\t\tconst filepath = \"/vrep/ca/ca.crt\"\n\t\tcert, err := ioutil.ReadFile(filepath)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"Error opening ca.crt: %q %v\", filepath, err)\n\t\t}\n\t\tcaCertPool := x509.NewCertPool()\n\t\tcaCertPool.AppendCertsFromPEM(cert)\n\n\t\treturn &http.Client{\n\t\t\tTransport: &http.Transport{\n\t\t\t\tTLSClientConfig: &tls.Config{\n\t\t\t\t\tRootCAs: caCertPool},\n\t\t\t},\n\t\t}, nil\n\t}\n\treturn &http.Client{\n\t\tTransport: &http.Transport{\n\t\t\tTLSClientConfig: &tls.Config{\n\t\t\t\tInsecureSkipVerify: true},\n\t\t},\n\t}, nil\n}\n\nfunc CheckMandatoryParameter(mandatoryValue *string, key string, getFunc func(string) string) error {\n\tvalue := getFunc(key)\n\tif len(value) == 0 {\n\t\treturn fmt.Errorf(\"mandatory parameter %q is not set\", key)\n\t}\n\t*mandatoryValue = value\n\treturn nil\n}\n\nfunc CheckArtifactKind(artifactKind string) (err error) {\n\tswitch artifactKind {\n\tcase\n\t\t\"model\",\n\t\t\"dataset\",\n\t\t\"other\":\n\t\treturn nil\n\t}\n\treturn fmt.Errorf(\"artifactKind should be \\\"{model,dataset,other}\\\", got %q\", artifactKind)\n}\n\nfunc CreateArtifactEndpoint(prefix string, path string, suffix string) string {\n\tendpoint := fmt.Sprintf(\"%v/app/ml-api/api/%v/%v\", prefix, path, suffix)\n\treturn endpoint\n}\n\nfunc createDataLakePath(path string) string {\n\tif strings.HasPrefix(path, \"/\") {\n\t\tpath = strings.TrimLeft(path, \"/\")\n\t}\n\treturn path\n}\n\nfunc createArtifactFullURI(connectionID string, path string) string {\n\tif strings.HasPrefix(path, \"/\") {\n\t\tpath = strings.TrimLeft(path, \"/\")\n\t}\n\treturn fmt.Sprintf(\"dh-dl://%v/%v\", connectionID, path)\n}\n\nfunc createDataLakeConnectionEndpoint(dataLakeCon vsystemclient.DLContentData) Connection {\n\tvar protocol string\n\tswitch dataLakeCon.Protocol {\n\tcase \"webhdfs\":\n\t\tprotocol = \"http\"\n\tcase \"swebhdfs\":\n\t\tprotocol = \"https\"\n\t}\n\treturn Connection{\n\t\tinternalCon: fmt.Sprintf(\"%v://%v:%v\", protocol, dataLakeCon.Host, dataLakeCon.Port),\n\t\texternalCon: fmt.Sprintf(\"%v://%v:%v\", protocol, dataLakeCon.PublicHost, dataLakeCon.PublicPort),\n\t}\n}\n\nfunc createURIForSDL(endpoint string, URL string) string {\n\treturn fmt.Sprintf(\"%v/webhdfs/v1/%v\", endpoint, URL)\n}\n\nfunc createURIForCreatingFileOnSDL(uri string) string {\n\treturn fmt.Sprintf(\"%v?op=CREATE&overwrite=false\", uri)\n}\n\nfunc executeSDLRequestWithHDFS(meta *requestSDLInfos) error {\n\t// create file and write file\n\n\treq, err := http.NewRequest(meta.httpMethod, meta.endpoint, meta.payload)\n\tif nil != err {\n\t\treturn fmt.Errorf(\"http request creation error: %v\", err)\n\t}\n\treq.Close = true\n\n\treq.Header.Add(\"Authorization\", meta.bearerToken)\n\treq.Header.Add(\"Content-Type\", meta.contentType)\n\n\tif nil == internalClient {\n\t\treturn errors.New(\"internalClient is not initialized\")\n\t}\n\n\tres, err := internalClient.Do(req)\n\tif nil != err {\n\t\treturn fmt.Errorf(\"hdfs create: %v\", err)\n\t}\n\n\tio.Copy(ioutil.Discard, res.Body)\n\tdefer res.Body.Close()\n\n\tif res.StatusCode != int(meta.expectedStatusCode) {\n\t\treturn fmt.Errorf(\"response status %q for request %v : %v\", res.Status, meta.httpMethod, meta.endpoint)\n\t}\n\n\treturn nil\n}\n\nfunc createExistingArtifactRequestMetaDataNew(getFunction func(string) string, executionID string, path string) (*ArtifactPostRequestMetaData, *artifactConfigs, *operatorConfigs, error) {\n\treturn createArtifactRequestMetaData(getFunction, executionID, 0, path)\n}\n\nfunc createNewArtifactRequestMetaDataNew(getFunction func(string) string, executionID string, counterSuffix uint64) (*ArtifactPostRequestMetaData, *artifactConfigs, *operatorConfigs, error) {\n\treturn createArtifactRequestMetaData(getFunction, executionID, counterSuffix, \"\")\n}\n\nfunc generateArtifactConfiguration(getFunction func(string) string) (*artifactConfigs, *operatorConfigs, error) {\n\tvar (\n\t\tinAPIVersion     string\n\t\tinOperatorName   string\n\t\tinService        string\n\t\tinVersionControl string\n\n\t\tinArtifactName        string\n\t\tinArtifactKind        string\n\t\tinArtifactDescription string\n\t\tinArtifactPrefix      string\n\t\tinArtifactSuffix      string\n\t)\n\n\tconst (\n\t\tcurrentAPIVersion = \"v1\"\n\t)\n\n\tif err := CheckMandatoryParameter(&inAPIVersion, \"apiVersion\", GetString); nil != err {\n\t\treturn nil, nil, err\n\t}\n\tif err := CheckMandatoryParameter(&inOperatorName, \"processName\", GetString); nil != err {\n\t\treturn nil, nil, err\n\t}\n\tif err := CheckMandatoryParameter(&inService, \"service\", GetString); nil != err {\n\t\treturn nil, nil, err\n\t}\n\tif err := CheckMandatoryParameter(&inVersionControl, \"versionControl\", GetString); nil != err {\n\t\treturn nil, nil, err\n\t}\n\n\tswitch inVersionControl {\n\tcase \"conf\", \"auto\":\n\t\tif err := CheckMandatoryParameter(&inArtifactName, \"artifactName\", GetString); nil != err {\n\t\t\treturn nil, nil, errors.New(\"mandatory parameter \\\"artifactName\\\" is not set\")\n\t\t}\n\t\tif err := CheckMandatoryParameter(&inArtifactKind, \"artifactKind\", GetString); nil != err {\n\t\t\treturn nil, nil, errors.New(\"mandatory parameter \\\"artifactKind\\\" is not set\")\n\t\t}\n\t\tinArtifactDescription = GetString(\"description\")\n\t\tinArtifactPrefix = GetString(\"prefix\")\n\t\tinArtifactSuffix = GetString(\"suffix\")\n\tcase \"input\":\n\t\tif err := CheckMandatoryParameter(&inArtifactName, \"artifactName\", getFunction); nil != err {\n\t\t\treturn nil, nil, errors.New(\"mandatory parameter \\\"artifactName\\\" is not set\")\n\t\t}\n\t\tif err := CheckMandatoryParameter(&inArtifactKind, \"artifactKind\", getFunction); nil != err {\n\t\t\treturn nil, nil, errors.New(\"mandatory parameter \\\"artifactKind\\\" is not set\")\n\t\t}\n\t\tinArtifactDescription = getFunction(\"description\")\n\t\tinArtifactPrefix = getFunction(\"prefix\")\n\t\tinArtifactSuffix = getFunction(\"suffix\")\n\tdefault:\n\t\treturn nil, nil, fmt.Errorf(\"versionControl should be \\\"{auto,conf,input}\\\", got %q\", inVersionControl)\n\t}\n\n\tif err := CheckArtifactKind(inArtifactKind); nil != err {\n\t\treturn nil, nil, err\n\t}\n\n\tif inAPIVersion != currentAPIVersion {\n\t\treturn nil, nil, fmt.Errorf(\"apiVersion should be %q, got %q\", currentAPIVersion, inAPIVersion)\n\t}\n\n\treturn &artifactConfigs{\n\t\t\tendpoint:              CreateArtifactEndpoint(gEndpoint, inAPIVersion, \"artifacts\"),\n\t\t\tinAPIVersion:          inAPIVersion,\n\t\t\tinOperatorName:        GetString(\"processName\"),\n\t\t\tinArtifactName:        inArtifactName,\n\t\t\tinArtifactKind:        inArtifactKind,\n\t\t\tinArtifactDescription: inArtifactDescription,\n\t\t\tinArtifactPrefix:      inArtifactPrefix,\n\t\t\tinArtifactSuffix:      inArtifactSuffix,\n\t\t}, &operatorConfigs{\n\t\t\tservice: inService,\n\t\t}, nil\n}\n\nfunc createURIFromConfig(inArtifact *artifactConfigs, executionID string, counterSuffix uint64) string {\n\tvar uniqueIdentifier = fmt.Sprintf(\"%v%v_%v%v\", inArtifact.inArtifactPrefix, inArtifact.inOperatorName, counterSuffix, inArtifact.inArtifactSuffix)\n\treturn createArtifactURI(executionID, uniqueIdentifier)\n}\n\nfunc createArtifactRequestMetaData(getFunction func(string) string, executionID string, counterSuffix uint64, path string) (*ArtifactPostRequestMetaData, *artifactConfigs, *operatorConfigs, error) {\n\n\tinArtifact, operatorConfigs, err := generateArtifactConfiguration(getFunction)\n\tif nil != err {\n\t\treturn nil, nil, nil, err\n\t}\n\n\tvar inPath string\n\tswitch path {\n\tcase \"\":\n\t\t// createNewArtifactRequestMetaData\n\t\tinPath = createURIFromConfig(inArtifact, executionID, counterSuffix)\n\tdefault:\n\t\t// createExistingArtifactRequestMetaData\n\t\tinPath = path\n\t}\n\n\treturn &ArtifactPostRequestMetaData{\n\t\tName:        inArtifact.inArtifactName,\n\t\tKind:        inArtifact.inArtifactKind,\n\t\tURI:         inPath,\n\t\tDescription: inArtifact.inArtifactDescription,\n\t\tType:        \"EXECUTION\",\n\t\tExecutionID: executionID,\n\t}, inArtifact, operatorConfigs, nil\n}\n\nfunc createArtifactURI(executionID string, uniqueIdentifier string) string {\n\tconst fixedPath = \"shared/ml/artifacts/executions\"\n\treturn fmt.Sprintf(\"%v/%v/%v\", fixedPath, executionID, uniqueIdentifier)\n}\n\nfunc ProcessErrorSetup(operation string, err error) {\n\tgOperatorConfig.Operation = operation\n\tvsystemclient.ProcessError(err, Errorf, OutError, gOperatorConfig, GetString(\"processName\"), \"Setup\")\n}\n\nfunc ProcessErrorInArtifact(operation string, err error) {\n\tgOperatorConfig.Operation = operation\n\tvsystemclient.ProcessError(err, Errorf, OutError, gOperatorConfig, GetString(\"processName\"), \"InArtifact\")\n}\n\nvar gOperatorConfig = vsystemclient.OperatorConfig{\n\tOperatorName: \"ArtifactProducer\",\n\tOperation:    \"artifact production\",\n\tOperatorPath: \"com.sap.ml.artifact.producer\",\n}\n\ntype operatorConfigs struct {\n\tservice string\n}\n\ntype Connection struct {\n\tinternalCon string\n\texternalCon string\n}\n\ntype artifactConfigs struct {\n\tendpoint              string\n\tinAPIVersion          string\n\tinOperatorName        string\n\tinArtifactName        string\n\tinArtifactKind        string\n\tinArtifactDescription string\n\tinArtifactPrefix      string\n\tinArtifactSuffix      string\n}\n\ntype requestSDLInfos struct {\n\thttpMethod         string\n\tendpoint           string\n\tbearerToken        string\n\tpayload            io.Reader\n\tcontentType        string\n\texpectedStatusCode int\n}\n"
				}
			},
			"name": "artifactproducer1"
		},
		"constantgenerator11": {
			"component": "com.sap.util.constantGenerator",
			"metadata": {
				"label": "Constant Generator",
				"x": 17,
				"y": 72,
				"height": 80,
				"width": 120,
				"extensible": true,
				"config": {
					"content": "SELECT * FROM \"SEP_CPQ\".\"Training_input2\""
				}
			},
			"name": "constantgenerator1"
		},
		"saphanaclient11": {
			"component": "com.sap.hana.client2",
			"metadata": {
				"label": "SAP HANA Client",
				"x": 201.99999904632568,
				"y": 72,
				"height": 80,
				"width": 120,
				"config": {
					"connection": {
						"configurationType": "Configuration Manager",
						"connectionID": "HANADB"
					},
					"tableName": "\"SEP_CPQ\".\"Training_input2\"",
					"csvHeader": "Ignore",
					"tableColumns": [
						{
							"name": "\"QUOTE_ID\"",
							"type": "NVARCHAR",
							"size": 8
						},
						{
							"name": "\"CUSTOMER_NAME\"",
							"type": "NVARCHAR",
							"size": 60
						},
						{
							"name": "\"INDUSTRY\"",
							"type": "NVARCHAR",
							"size": 1
						},
						{
							"name": "\"CREATION_DATE\"",
							"type": "NVARCHAR",
							"size": 25
						},
						{
							"name": "\"COUNTRY\"",
							"type": "NVARCHAR",
							"size": 10
						},
						{
							"name": "\"PRODUCT_NAME\"",
							"type": "NVARCHAR",
							"size": 40
						},
						{
							"name": "\"MODEL\"",
							"type": "NVARCHAR",
							"size": 40
						},
						{
							"name": "\"SUPPLY_VOLTAGE_A\"",
							"type": "NVARCHAR",
							"size": 60
						},
						{
							"name": "\"COM_QUANTITY\"",
							"type": "INTEGER"
						},
						{
							"name": "\"STATUS\"",
							"type": "NVARCHAR",
							"size": 1
						},
						{
							"name": "\"LISTPRICE\"",
							"type": "DOUBLE"
						},
						{
							"name": "\"MANUFACTURING_COST\"",
							"type": "DOUBLE"
						},
						{
							"name": "\"QUOTED_PRICE\"",
							"type": "DOUBLE"
						}
					]
				}
			},
			"name": "saphanaclient1"
		},
		"tostringconverter1": {
			"component": "com.sap.util.toStringConverter",
			"metadata": {
				"label": "ToString Converter",
				"x": 386.99999809265137,
				"y": 87,
				"height": 50,
				"width": 50,
				"config": {}
			}
		},
		"saphanaclient1": {
			"component": "com.sap.hana.client2",
			"metadata": {
				"label": "SAP HANA Client",
				"x": 807,
				"y": -14,
				"height": 80,
				"width": 120,
				"config": {
					"connection": {
						"configurationType": "Configuration Manager",
						"connectionID": "HANADB"
					},
					"tableName": "\"SEP_CPQ\".\"TA_IMC_CPQ_TRAINING_OPTIMAL_PRICE\"",
					"tableColumns": [
						{
							"name": "\"QUOTE_ID\"",
							"type": "NVARCHAR",
							"size": 8
						},
						{
							"name": "\"REFRESH_MONTH\"",
							"type": "NVARCHAR",
							"size": 6
						},
						{
							"name": "\"CUSTOMER_NAME\"",
							"type": "NVARCHAR",
							"size": 60
						},
						{
							"name": "\"INDUSTRY\"",
							"type": "NVARCHAR",
							"size": 1
						},
						{
							"name": "\"CREATION_DATE\"",
							"type": "NVARCHAR",
							"size": 20
						},
						{
							"name": "\"COUNTRY\"",
							"type": "NVARCHAR",
							"size": 10
						},
						{
							"name": "\"PRODUCT_NAME\"",
							"type": "NVARCHAR",
							"size": 40
						},
						{
							"name": "\"QUANTITY\"",
							"type": "INTEGER"
						},
						{
							"name": "\"LISTPRICE\"",
							"type": "DOUBLE"
						},
						{
							"name": "\"MANUFACTURING_COST\"",
							"type": "DOUBLE"
						},
						{
							"name": "\"QUOTED_PRICE\"",
							"type": "DOUBLE"
						},
						{
							"name": "\"OPTIMUM_PRICE\"",
							"type": "DOUBLE"
						},
						{
							"name": "\"PROB_WIN_OPTIMAL\"",
							"type": "DOUBLE"
						},
						{
							"name": "\"OPTIMAL_LOWER\"",
							"type": "DOUBLE"
						},
						{
							"name": "\"OPTIMAL_HIGHER\"",
							"type": "DOUBLE"
						},
						{
							"name": "\"PROB_WIN_ACTUAL\"",
							"type": "DOUBLE"
						}
					],
					"initTable": "Drop (Cascade)"
				}
			}
		}
	},
	"groups": [
		{
			"name": "group1",
			"nodes": [
				"python3operator1111"
			],
			"metadata": {
				"description": "Group"
			},
			"tags": {
				"CP": ""
			}
		}
	],
	"connections": [
		{
			"metadata": {
				"points": "1136.9999933242798,112 1164.9999928474426,112 1164.9999928474426,121 1192.9999923706055,121"
			},
			"src": {
				"port": "outstring",
				"process": "tostringconverter21"
			},
			"tgt": {
				"port": "artifactId",
				"process": "python3operator21"
			}
		},
		{
			"metadata": {
				"points": "906.9999952316284,163 934.9999947547913,163 934.9999947547913,103 962.9999942779541,103"
			},
			"src": {
				"port": "outArtifact",
				"process": "artifactproducer11"
			},
			"tgt": {
				"port": "inbody",
				"process": "tomessageconverter11"
			}
		},
		{
			"metadata": {
				"points": "1021.9999942779541,112 1049.999993801117,112 1049.999993801117,121 1077.9999933242798,121"
			},
			"src": {
				"port": "out",
				"process": "tomessageconverter11"
			},
			"tgt": {
				"port": "inmessage",
				"process": "tostringconverter21"
			}
		},
		{
			"metadata": {
				"points": "645.999997138977,129 673.9999966621399,129 673.9999966621399,125.5 749.9999957084656,125.5 749.9999957084656,163 777.9999952316284,163"
			},
			"src": {
				"port": "modelBlob",
				"process": "python3operator1111"
			},
			"tgt": {
				"port": "inArtifact",
				"process": "artifactproducer11"
			}
		},
		{
			"metadata": {
				"points": "141,112 168.99999952316284,112 168.99999952316284,103 196.99999904632568,103"
			},
			"src": {
				"port": "out",
				"process": "constantgenerator11"
			},
			"tgt": {
				"port": "sql",
				"process": "saphanaclient11"
			}
		},
		{
			"metadata": {
				"points": "325.9999990463257,112 353.9999985694885,112 353.9999985694885,103 381.99999809265137,103"
			},
			"src": {
				"port": "result",
				"process": "saphanaclient11"
			},
			"tgt": {
				"port": "ininterface",
				"process": "tostringconverter1"
			}
		},
		{
			"metadata": {
				"points": "440.99999809265137,112 468.9999976158142,112 468.9999976158142,120 516.999997138977,120"
			},
			"src": {
				"port": "outstring",
				"process": "tostringconverter1"
			},
			"tgt": {
				"port": "input2",
				"process": "python3operator1111"
			}
		},
		{
			"metadata": {
				"points": "645.999997138977,111 724,111 724,35 802,35"
			},
			"src": {
				"port": "output2",
				"process": "python3operator1111"
			},
			"tgt": {
				"port": "data",
				"process": "saphanaclient1"
			}
		}
	],
	"inports": {},
	"outports": {}
}