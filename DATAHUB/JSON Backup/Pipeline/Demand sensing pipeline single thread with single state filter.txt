{
	"properties": {},
	"groupResources": {},
	"description": "ML_PIPELINE_DEMAND_SENSING_JUPYTER_STATES",
	"processes": {
		"python3operator1111": {
			"component": "com.sap.system.python3Operator",
			"metadata": {
				"label": "DemandSensing_Training",
				"x": 577.9999961853027,
				"y": 110,
				"height": 80,
				"width": 120,
				"extensible": true,
				"config": {
					"script": "#import Libraries\nimport io\nimport importlib\nimport pandas as pd\nimport os\nimport datetime\nimport pandas as pd\nimport numpy as np\nimport ast\nimport time\nfrom configparser import ConfigParser\nfrom pathlib import Path\nfrom concurrent.futures import ProcessPoolExecutor\nfrom datetime import date\nfrom fbprophet import Prophet\nfrom datetime import date\nfrom configparser import ConfigParser\nfrom pathlib import Path\nfrom itertools import repeat\nimport matplotlib.pyplot as plt\n\"\"\"\n#declare variables\nGROUPBY_LEVEL = ['retailer', 'state', 'brand', 'ppg']\nTARGET = \"FillMean\"\nDATE_COLUMN = \"week_ending_date\"\n# Add additional regressors to the forecast\nADDITIONAL_REGRESSORS = ['food_cpi_nat_mth', 'snap_cost_st_mth']\nREGRESSOR_LAG = {'snap_cost_st_mth': 1}\n\n# Establish training/test windows\nTRAIN_START = pd.to_datetime('2019-01-01').date()\nTRAIN_END = pd.to_datetime('2020-06-30').date()\nTEST_START = pd.to_datetime('2020-09-30').date()\nTEST_END = pd.to_datetime('2020-12-31').date()\n# To speed up running, set to desired sample size if not set to 0\nSAMPLE = 2\n# Set logistic growth function cap\nCAP_PERCENTILE = 95\n# Future Period\nFUTURE_PERIOD = 25\n# model parameters\nOPTIM_PARAM = {\"growth\": \"logistic\", \"seasonality_prior_scale\": 0.1}\n\"\"\"\n\n\n\"\"\"\n#function\n\"\"\"\ndef forecast(group, df_add, cust_df):\n    api.send(\"output2\",'Entering Forecast ')\n    index, df_group = group\n    group = group[0]\n\n    p = OPTIM_PARAM.copy()\n    # Keep track of how long it takes to run for 1 group\n    start_time = time.time()\n\n    # Keep track of data throughout run\n    data = []\n    excluded = []\n    for i in range(len(GROUPBY_LEVEL)):\n        data.append(index[i])\n\n    # Make sure we do not have NaN in additional regressor columns\n    df_group = df_group.sort_values('ds')\n    train_df = df_group.loc[df_group['ds'] <= TRAIN_END]\n\n    max_val = abs(np.percentile(train_df[\"y\"], CAP_PERCENTILE))\n    #min_val = abs(np.percentile(train_df[\"y\"], 5))\n    train_df[\"cap\"] = max_val\n    #train_df[\"floor\"] = min_val\n\n    # Initialize model\n    m = Prophet(holidays=cust_df, **p)\n\n    # Add holiday and additional regressors\n    m.add_country_holidays(country_name='US')\n\n    # Add additional regressors\n    if ADDITIONAL_REGRESSORS != []:\n        for regressor in ADDITIONAL_REGRESSORS:\n            m.add_regressor(regressor)\n    # Fit model\n    try:\n        m.fit(train_df)\n        # Create future dataframe and predict\n        future = m.make_future_dataframe(periods=FUTURE_PERIOD, include_history=False, freq='7D')\n        future[\"cap\"] = max_val\n        future['ds'] = pd.to_datetime(future['ds']).dt.date\n\n\n        if ADDITIONAL_REGRESSORS != []:\n            future[\"state\"] = group[1]\n            df_add['ds'] = pd.to_datetime(df_add['ds']).dt.date\n            future = pd.merge(future, df_add, on=['ds', 'state'], how='left')\n\n\n        forecast = m.predict(future)\n        forecast['ds'] = pd.to_datetime(forecast['ds']).dt.date\n        #print('Executing Forecast Routine')\n        df_final = forecast.loc[(forecast['ds'] >= TEST_START) & (forecast['ds'] <= TEST_END)]\n        df_final = df_final[['ds', 'yhat'] + ADDITIONAL_REGRESSORS]\n        df_final.rename(columns={'ds': DATE_COLUMN, 'yhat': TARGET}, inplace=True)\n        for i in range(len(GROUPBY_LEVEL)):\n            df_final[GROUPBY_LEVEL[i]] = group[i]\n\n        df_final = df_final[GROUPBY_LEVEL + ADDITIONAL_REGRESSORS + [DATE_COLUMN, TARGET]]\n        #print('time : ' + str(time.time() - start_time))\n    except Exception as e:\n        excluded.append(group[:len(GROUPBY_LEVEL)])\n        df_final = pd.DataFrame()\n        print(e)\n\n    #return df_final, excluded\n    return df_final #AK\ndef main():\n    \n\n    \n    def on_input(msg1, msg2):\n        import pandas as pd\n        import io\n        api.send(\"output1\",'line 123')\n        GROUPBY_LEVEL = ['retailer', 'state', 'brand', 'ppg']\n        TARGET = \"FillMean\"\n        DATE_COLUMN = \"week_ending_date\"\n        # Add additional regressors to the forecast\n        ADDITIONAL_REGRESSORS = ['food_cpi_nat_mth', 'snap_cost_st_mth']\n        REGRESSOR_LAG = {'snap_cost_st_mth': 1}\n        \n        # Keep track of data throughout run\n        data = []\n        excluded = []\n        df_final_out = pd.DataFrame()\n        count=0\n        try_count = 0\n    \n        # Establish training/test windows\n        TRAIN_START = pd.to_datetime('2019-01-01').date()\n        TRAIN_END = pd.to_datetime('2020-06-30').date()\n        TEST_START = pd.to_datetime('2020-09-30').date()\n        TEST_END = pd.to_datetime('2020-12-31').date()\n        # To speed up running, set to desired sample size if not set to 0\n        SAMPLE = 0\n        # Set logistic growth function cap\n        CAP_PERCENTILE = 95\n        # Future Period\n        FUTURE_PERIOD = 25\n        # model parameters\n        OPTIM_PARAM = {\"growth\": \"logistic\", \"seasonality_prior_scale\": 0.1}\n        \n\n        def custom_fillna(series):\n            if series.dtype is pd.np.dtype(float):\n                return series.fillna(0)\n            elif series.dtype is pd.np.dtype('int32'):\n                return series.fillna(0)\n            elif series.dtype is pd.np.dtype('int64'):\n                return series.fillna(0)\n            elif series.dtype is pd.np.dtype(str):\n                return series.fillna(0)  \n            elif series.dtype is pd.np.dtype(object):\n                return series.fillna('')  \n            else:\n                return series\n    \n        \n        # Obtain data\n        # input table from SEP_COVIDEXT.Z_SEP.AnalyticalModels.SCM.DemandForecasting.CovidExternal::TA_SCM_COVID_FULL_SAMPLE_TRAIN\n        # format data frame as per data type in source table\n        \n        #df_sample_train = pd.read_csv(io.StringIO(msg1.body), sep=\",\")\n        df =  pd.read_json(io.StringIO(msg2))\n        #checking null values and replace accordingly\n        df = df.apply(custom_fillna)\n        df['week_ending_date'] =  df['week_ending_date'].astype(str)\n        #data = df.values.tolist()\n        #api.send(\"output1\",data)\n        \"\"\"\n        df = pd.DataFrame()\n        df['week_ending_date'] = df_sample_train.iloc[:,0]\n        df['retailer'] = df_sample_train.iloc[:,1]\n        df['state'] = df_sample_train.iloc[:,2]\n        df['business'] = df_sample_train.iloc[:,3]\n        df['category'] = df_sample_train.iloc[:,4]\n        df['brand'] = df_sample_train.iloc[:,5]\n        df['ppg'] = df_sample_train.iloc[:,6]\n        df['week_of_year'] = df_sample_train.iloc[:,7].astype('int64')\n        df['pos_qty_ty'] = df_sample_train.iloc[:,8].astype(float)\n        df['pos_dollar_ty'] = df_sample_train.iloc[:,9].astype(float)\n        df['FillMean'] = df_sample_train.iloc[:,10].astype(float)\n        \"\"\"\n        \n        # input table from SEP_COVIDEXT.Z_SEP.AnalyticalModels.SCM.DemandForecasting.CovidExternal::TA_SCM_COVID_EXT_WEEKLY_MODEL_TRAIN\n        # format data frame as per data type in source table\n        \n        \n        #df_external = pd.read_csv(io.StringIO(msg2.body), sep=\",\")\n        #checking null values and replace accordingly\n        df_add_external =  pd.read_json(io.StringIO(msg1))\n        df_add_external = df_add_external.apply(custom_fillna)\n        df_add_external['date'] =  df_add_external['date'].astype(str)\n        #data1 = df_add_external.values.tolist()\n        #api.send(\"output2\",data1)\n        \"\"\"\n        df_add_external = pd.DataFrame()\n        df_add_external['date'] = df_external.iloc[:,0]\n        df_add_external['state'] = df_external.iloc[:,1]\n        df_add_external['state_initial'] = df_external.iloc[:,2]\n        df_add_external['AT_adj'] = df_external.iloc[:,3].astype(float)\n        df_add_external['food_cpi_nat_mth'] = df_external.iloc[:,4].astype(float)\n        df_add_external['snap_cost_st_mth'] = df_external.iloc[:,5].astype(float)\n        df_add_external['allbed_mean'] = df_external.iloc[:,6].astype(float)\n        df_add_external['confirmed_infections'] = df_external.iloc[:,7].astype(float)\n        df_add_external['deaths_mean'] = df_external.iloc[:,8].astype(float)\n        df_add_external['est_infections_mean'] = df_external.iloc[:,9].astype(float)\n        df_add_external['mobility_composite_wors'] = df_external.iloc[:,10].astype(float)\n        df_add_external['states_on_stay_home'] = df_external.iloc[:,11].astype('int64')\n        df_add_external['states_on_travel_limit'] = df_external.iloc[:,12].astype('int64')\n        df_add_external['states_on_any_business'] = df_external.iloc[:,13].astype('int64')\n        df_add_external['states_on_all_non-ess_business'] = df_external.iloc[:,14].astype('int64')\n        df_add_external['states_on_any_gathering_restrict'] = df_external.iloc[:,15].astype('int64')\n        df_add_external['states_on_educational_fac'] = df_external.iloc[:,16].astype('int64')\n       \n        api.send(\"output2\", 'Line 101')\n    \n        api.send(\"output2\", 'Line 341')\n        \"\"\"\n        def get_end_date_from_week(year,week,day):\n    \n            #Calculates first day and last day of week, given a year and week\n            first_day = datetime.datetime.strptime(f'{year}-W{int(week )- 1}-1', '%Y-W%W-%w').date()\n            last_day = first_day + datetime.timedelta(days=day)\n            return last_day\n        \n        def get_week_number_from_end_date(date_obj):\n            #Calculates week number in year given a date.\n            week_number = date_obj.isocalendar()[1]\n            return week_number   \n        \n        def data_imputation(df):\n            # Fill in missing week numbers\n            df['week_of_year'] = df[DATE_COLUMN].apply(lambda x: get_week_number_from_end_date(x))\n            df[DATE_COLUMN] = pd.to_datetime(df[DATE_COLUMN]).dt.date\n            grouped = df.groupby(GROUPBY_LEVEL)\n            data1 =  grouped.values.tolist()\n            api.send(\"output2\",data1)\n            api.send(\"output2\",'Line 165')\n            subset_list = []\n            for name, group in grouped:\n                subset_df = group.copy()\n                subset_df = subset_df.set_index(DATE_COLUMN)\n                # Imputation approach using mean value\n                subset_df = subset_df.assign(imputed_qty=subset_df[TARGET].fillna(subset_df[TARGET].mean()))\n                subset_df = subset_df.reset_index()\n                subset_list.append(subset_df)\n            imputed_df = pd.concat(subset_list)\n            #imputed_df.to_csv(DATA_PATH+'processed/imputed_data.csv', index=False)\n            return imputed_df\n            api.send(\"output2\", 'Line 130')\n    \n    \n        def add_external_data(df, df_add):\n            #Takes client data and external data\n            \n    \n            df_add = df_add[['date', 'state_initial'] + ADDITIONAL_REGRESSORS]\n            df_add.rename(columns={'date': 'week_ending_date', 'state_initial': 'state'}, inplace=True)\n            if REGRESSOR_LAG != {}:\n                for k, v in REGRESSOR_LAG.items():\n                    df_add[k] = df_add.groupby('state')[k].shift(v)\n            df = pd.merge(df, df_add, on=['week_ending_date', 'state'], how='left')\n            df_add[DATE_COLUMN] = pd.to_datetime(df_add[DATE_COLUMN]).dt.date\n            df_add.rename(columns={DATE_COLUMN: 'ds'}, inplace=True)\n            return df, df_add\n            api.send(\"output2\", 'Line 146')\n    \n        def select_data(df):\n            if ADDITIONAL_REGRESSORS:\n                df = df[[DATE_COLUMN] + GROUPBY_LEVEL + ADDITIONAL_REGRESSORS + [TARGET]]\n            else:\n                df = df[[DATE_COLUMN] + GROUPBY_LEVEL + [TARGET]]\n            print('Selected data columns.')\n    \n            return df\n    \n        def select_sample(df):\n            df_sum = df.groupby(GROUPBY_LEVEL)[[TARGET]].sum().reset_index().rename(columns={TARGET: 'total_qty'})\n            top_100 = df_sum.nlargest(columns='total_qty', n=SAMPLE)\n            top_100 = top_100.drop(['total_qty'], axis=1)\n            df = pd.merge(top_100, df, how='inner', on=GROUPBY_LEVEL)\n            print('Chose top {} samples.'.format(SAMPLE))\n            return df\n        \n        def load_holiday_calendar():\n            # Builds a holiday calendar.\n            # New year's day\n            newyear = pd.DataFrame({\n            'holiday': 'newyear',\n            'ds': pd.to_datetime(['2019-01-01','2020-01-01']),\n            })\n            # Martin Luther King Jr. Day\n            MLK_day = pd.DataFrame({\n            'holiday': 'MLK_day',\n            'ds': pd.to_datetime(['2019-01-21','2020-01-20']),\n            })\n            # March Madness\n            march_madness = pd.DataFrame({\n            'holiday': 'march_madness',\n            'ds': pd.to_datetime(['2018-03-24','2018-03-31','2019-03-30','2019-04-06','2020-03-28','2020-04-04', '2021-03-27','2021-04-03']),\n            })\n            # Superbowl\n            superbowls = pd.DataFrame({\n            'holiday': 'superbowl',\n            'ds': pd.to_datetime(['2018-01-27','2018-02-03', '2019-01-26','2019-02-02', '2020-01-25','2020-02-01','2021-01-30','2021-02-06']),\n            })\n            # Lent\n            lent = pd.DataFrame({\n            'holiday': 'lent',\n            'ds': pd.to_datetime(['2018-02-17', '2018-02-24','2018-03-03','2018-03-10','2018-03-17','2018-03-24',\n                                '2019-03-09', '2019-03-16','2019-03-23','2019-03-30','2019-04-06','2019-04-13',\n                                '2020-02-29', '2020-03-07', '2020-03-14','2020-03-21','2020-03-28','2020-04-04',\n                                '2021-02-20', '2021-02-27', '2021-03-06','2021-03-13','2021-03-20','2021-03-27']),\n            })\n            # Easter (Wednesday – Easter Friday)\n            easter = pd.DataFrame({\n            'holiday': 'easter',\n            'ds': pd.to_datetime(['2018-03-31', '2019-04-20', '2020-04-11','2021-04-03']),\n            })\n            # Memorial day\n            memorial_day = pd.DataFrame({\n            'holiday': 'memorial_day',\n            'ds': pd.to_datetime(['2019-05-27', '2020-05-25']),\n            })\n            # Independence day\n            indep_day = pd.DataFrame({\n            'holiday': 'indep_day',\n            'ds': pd.to_datetime(['2019-07-04', '2020-07-03']),\n            })\n            # Labor day\n            labor_day = pd.DataFrame({\n            'holiday': 'indep_day',\n            'ds': pd.to_datetime(['2019-09-02', '2020-09-07']),\n            })\n            # Halloween\n            halloween = pd.DataFrame({\n            'holiday': 'halloween',\n            'ds': pd.to_datetime(['2018-10-27', '2019-10-26', '2020-10-31','2021-10-30']),\n            })\n            # Veteran's day\n            veteran_day = pd.DataFrame({\n            'holiday': 'veteran_day',\n            'ds': pd.to_datetime(['2019-11-11', '2020-11-11']),\n            })\n            # Thanksgiving\n            thanksgiving = pd.DataFrame({\n            'holiday': 'thanksgiving',\n            'ds': pd.to_datetime(['2019-11-28', '2020-11-26']),\n            })\n            # Christmas\n            Christmas = pd.DataFrame({\n            'holiday': 'thanksgiving',\n            'ds': pd.to_datetime(['2019-12-25', '2020-12-25']),\n            })\n    \n            holidays_df = pd.concat((newyear, MLK_day, march_madness, superbowls, lent, easter, memorial_day, indep_day, labor_day, halloween, veteran_day, thanksgiving, Christmas))\n            return holidays_df\n            api.send(\"output2\", 'Line 238')\n    \n        def get_week_day(df):\n            df['ds'] = pd.to_datetime(df['ds']).dt.date\n            return df['ds'].iloc[0].weekday()\n    \n        def custom_holidays(week_day):\n            custom_holidays = load_holiday_calendar()\n            custom_holidays['week_no'] = custom_holidays['ds'].apply(lambda x: get_week_number_from_end_date(x))\n            custom_holidays['year'] = custom_holidays['ds'].apply(lambda x: int(x.strftime('%Y')))\n            custom_holidays['week_ending_date'] = custom_holidays.apply(\n                lambda x: get_end_date_from_week(x['year'], x['week_no'], week_day), 1)\n            custom_holidays.rename(columns={'ds': 'date', 'week_ending_date': 'ds'}, inplace=True)\n            custom_holidays = custom_holidays[['ds', 'holiday']]\n    \n            return custom_holidays\n    \n        def plot_mape(stats_df):\n            plt.style.use('ggplot')\n            first_edge, last_edge = stats_df['mape'].min(), stats_df['mape'].max()\n    \n            n_equal_bins = 60\n            bin_edges = np.linspace(start=first_edge, stop=last_edge, num=n_equal_bins + 1, endpoint=True)\n    \n            # Creating histogram\n            fig, ax = plt.subplots(figsize =(8, 4))\n            ax.hist(stats_df['mape'], bins = bin_edges,  color = (0.5,0.1,0.5,0.6))\n    \n            plt.title('MAPE distribution of forecast results.')\n    \n            # Save plot\n            plt.savefig(DATA_PATH+'mape_plot.png')\n    \n        def mean_absolute_percentage_error(y_true, y_pred):\n            y_true, y_pred = np.array(y_true), np.array(y_pred)\n            return np.mean(np.abs((y_true - y_pred)/ y_true)) * 100\n        \n\n        #api.send(\"output2\",'Line 390')\n    \n    #processing\n        \n        df, df_add_new = add_external_data(df, df_add_external)\n        api.send(\"output2\",'Line 395')\n        df_add_new['ds'] =  df_add_new['ds'].astype(str)\n        #data =  df.values.tolist()\n        #api.send(\"output2\",data)\n    \n        # Track how long end-to-end modeling takes\n        start_time = time.time()\n        #load data, date should be in first column\n        df[DATE_COLUMN] = pd.to_datetime(df[DATE_COLUMN]).dt.date\n        df[DATE_COLUMN] =  df[DATE_COLUMN].astype(str) #Convert Date to String for DI\n        # Load Additional Data\n        #df_add_new['ds'] = pd.to_datetime(df_add_new['ds']).dt.date\n        #df_add_new['ds'] =  df_add_new['ds'].astype(str) #Convert Date to String for DI\n        # Get relevant columns\n        df = select_data(df)\n        #data =  df.values.tolist()\n        #api.send(\"output2\",data)\n    \n        #get sample, if number provided, else run on full set\n        if SAMPLE:\n            df = select_sample(df)\n            api.send(\"output2\",'Line 416')\n        df.rename(columns={DATE_COLUMN: 'ds', TARGET: 'y'}, inplace=True)\n        #data =  df.values.tolist()\n        #api.send(\"output2\",data)\n\n        #get Weekday\n        week_day = get_week_day(df)\n        api.send(\"output2\",'Line 423')\n        #create custom holiday\n        cust_df_new = custom_holidays(week_day)\n        cust_df_new['ds'] =  cust_df_new['ds'].astype(str)\n        #data =  cust_df_new.values.tolist()\n        #api.send(\"output2\",data)\n    \n        df_add_new['ds'] =  df_add_new['ds'].astype(str)\n        #data =  df_add_new.values.tolist()\n        api.send(\"output2\",'Line 433')\n        #train, forecast, and get results\n        final_data = []\n        excluded_groups = []\n        # Rename to prophet's requirements\n        df.rename(columns={DATE_COLUMN: 'ds', TARGET: 'y'}, inplace=True)\n        # Group data to build individual models at each level\n        #grouped= pd.DataFrame()#Force grouped to a dataframe\n        #grouped1= pd.DataFrame()\n        grouped = df.groupby(GROUPBY_LEVEL) #To check if groupby() is working\n        grouped_ak = df.groupby(GROUPBY_LEVEL).count() #by AK\n        data = grouped_ak.values.tolist()\n        #api.send(\"output2\",data)\n        \n        \n        \"\"\"\n        with ProcessPoolExecutor(max_workers=4) as executor:#Not supported by DI Python operator . Tested on 30.10.2020\n            #results = map(forecast, grouped, repeat(df_add_new), repeat(cust_df_new))\n\t        results = executor.map(forecast, grouped, repeat(df_add_new), repeat(cust_df_new))\n        #api.send(\"output2\",len(list(results)))\n        \n        api.send(\"output2\",'Line 452')\n        #df_add_new = df_add_new.isna()\n        df_add_new = df_add_new.apply(custom_fillna)\n        df_add_new = df_add_new.values.tolist()\n        cust_df_new = cust_df_new.apply(custom_fillna)\n        cust_df_new = cust_df_new.values.tolist()\n        api.send(\"output2\",df_add_new)\n        api.send(\"output1\",cust_df_new)\n        \n        api.send(\"output2\",'Line 462')\n        df_output = forecast(grouped, df_add_new, cust_df_new)\n        \n        \n        for result in results:\n            api.send(\"output2\",'Line 467')\n            data, excluded = result\n            if len(excluded) > 0:\n                excluded_groups.append(excluded)\n            else:\n                final_data.append(data)\n        api.send(\"output2\",len(final_data))\n        df_output = pd.concat(final_data)        \n        #print('Total time taken: {}'.format(time.time() - start_time)) \n        #api.send(\"output2\", 'Success!!')\n        \"\"\"\n        api.send(\"output1\", 'line 489')\n        data2 = grouped_ak.values.tolist()\n        api.send(\"output1\",data2)\n        #api.send(\"output2\", api.Message(grouped.groups))\n        #for g in grouped.groups:\n         #   api.send(\"output1\", g)\n        count=0\n        lst=[]\n        start_time=time.time()\n        for g in grouped.groups:\n            \n            \n            count = count+1\n            \n            p = OPTIM_PARAM.copy()\n            # Keep track of how long it takes to run for 1 group\n            start_time = time.time()\n            \n            api.send(\"output1\", 'line 496')\n            \n            for i in range(len(GROUPBY_LEVEL)):\n                data.append(g[i])\n        \n            \n            # Make sure we do not have NaN in additional regressor columns\n            df_group = grouped.get_group(g)\n            lst.append(df_group.shape[0])\n            \n            df_group = df_group.sort_values('ds')\n            api.send(\"output1\", 'line 504')\n            # Set train/predict windows\n            train_df = df_group.loc[df_group['ds'] <= TRAIN_END]\n        \n            # Set cap for logistic function\n            max_val = abs(np.percentile(train_df[\"y\"], CAP_PERCENTILE))\n           # min_val = abs(np.percentile(train_df[\"y\"], 5))\n            api.send(\"output1\", 'line 511')\n            train_df[\"cap\"] = max_val\n            \n            #train_df[\"floor\"] = min_val\n        \n            # Initialize model\n            m = Prophet(holidays=cust_df_new, **p)\t\n            # Add holiday and additional regressors\n            m.add_country_holidays(country_name='US')\t\n        \n            # Add additional regressors\n            if ADDITIONAL_REGRESSORS != []:\n                for regressor in ADDITIONAL_REGRESSORS:\n                    m.add_regressor(regressor)\n            api.send(\"output1\", 'line 529')\n            train_df['ds']= train_df['ds'].astype(str)\n            data =  train_df.columns.values.tolist()+train_df.values.tolist()\n            #api.send(\"output1\", data)\n            \n            \n            \n       \n            \n            # Fit model\n            try:\n                #try_count = try_count+1\n                #api.send(\"output1\", try_count)\n                m.fit(train_df)\n                api.send(\"output1\", 'line 532')\n                # Create future dataframe and predict\n                future = m.make_future_dataframe(periods=FUTURE_PERIOD, include_history=False, freq='7D')\n                future[\"cap\"] = max_val\n                future['ds'] = pd.to_datetime(future['ds']).dt.date\t\t\n                api.send(\"output1\", 'line 537')\n                if ADDITIONAL_REGRESSORS != []:\n                    future[\"state\"] = g[1]\n                    df_add_new[\"ds\"] = pd.to_datetime(df_add_new['ds']).dt.date\n                    future = pd.merge(future, df_add_new, on=['ds', 'state'], how='left')\t\n                    \n                future = future.dropna()\n                forecast = m.predict(future)\n                forecast['ds'] = pd.to_datetime(forecast['ds']).dt.date\t\n                api.send(\"output1\", 'line 540')\n                df_final = forecast.loc[(forecast['ds'] >= TEST_START) & (forecast['ds'] <= TEST_END)]\n                df_final = df_final[['ds', 'yhat'] + ADDITIONAL_REGRESSORS]\n                df_final.rename(columns={'ds': DATE_COLUMN, 'yhat': TARGET}, inplace=True)\n                for i in range(len(GROUPBY_LEVEL)):\n                    df_final[GROUPBY_LEVEL[i]] = g[i]\n                api.send(\"output1\", 'line 546')\n                df_final = df_final[GROUPBY_LEVEL + ADDITIONAL_REGRESSORS + [DATE_COLUMN, TARGET]]\t\n                df_final_out = df_final_out.append(df_final)\n                data =  df_final.values.tolist()\n                #api.send(\"output1\", data)\n                del forecast, df_final, future, m, df_group, train_df\n            except Exception as e:\n                excluded.append(g[:len(GROUPBY_LEVEL)])\n                df_final = pd.DataFrame()\n                api.send(\"output1\", 'line 554')\n                data =  df_final.values.tolist()\n                api.send(\"output1\", data)\n                del df_final,  m, df_group, train_df\n                continue\n                \n           \n        api.send(\"output1\", 'line 558')\n        df_final_out['week_ending_date']= df_final_out['week_ending_date'].astype(str)\n        #data =  df_final_out.values.tolist()\n        data =  df_final_out.values.tolist()\n        api.send(\"output1\", count)\n        api.send(\"output1\", try_count)\n        #api.send(\"output3\", data)\n        f2 = '{},{},{},{},{},{},{},{}'# format Output_Seg\n        for j in data:\n            \n            api.send(\"output3\",f2.format(*j)+'\\n')\n        \"\"\"\n        \n        for j in df_final_out:\n            api.send(\"output1\", 'line 561')\n            api.send(\"output1\",df_final_out.format(*j)+'\\n')\n        \"\"\"\n        api.send(\"output1\", 'line 568')\n        \"\"\"\n        #Check which columns have NaN in pandas dataframe\n        df_nan = df_final_out.isna()\n        nan_columns = df_nan.any()\n        columns_with_nan = df_nan.columns[nan_columns].tolist()\n        api.send(\"output2\",'Line 469')\n        if len(columns_with_nan) > 0:\n            df_output = df_final_out.apply(custom_fillna)\n            data2 =  df_output.values.tolist()\n            api.send(\"output2\", data2)\n        else:\n            df_output  \n            data2 = df_output.values.tolist()\n            api.send(\"output2\", data2)\n        \n        \"\"\"\n        api.send(\"output2\",time.time()-start_time)\n        api.send(\"output2\",count)\n        api.send(\"output2\",np.mean(lst))\n    api.set_port_callback([\"input1\",\"input2\"], on_input)\nif __name__ == '__main__':\n    main()  \n    #df_output will write back to \n    #\"SEP_COVIDEXT\".\"Z_SEP.AnalyticalModels.SCM.DemandForecasting.CovidExternal::TA_SCM_COVID_LONGTERM_FORECAST_OUTPUT\""
				},
				"additionalinports": [
					{
						"name": "input2",
						"type": "string"
					},
					{
						"name": "input1",
						"type": "string"
					}
				],
				"additionaloutports": [
					{
						"name": "output2",
						"type": "message"
					},
					{
						"name": "output1",
						"type": "message"
					},
					{
						"name": "output3",
						"type": "message"
					}
				]
			},
			"name": "python3operator111"
		},
		"constantgenerator111": {
			"component": "com.sap.util.constantGenerator",
			"metadata": {
				"label": "Constant Generator",
				"x": 17,
				"y": 162,
				"height": 80,
				"width": 120,
				"extensible": true,
				"config": {
					"content": " select * from \"SEP_COVIDEXT\".\"Z_SEP.AnalyticalModels.SCM.DemandForecasting.CovidExternal::TA_SCM_COVID_EXT_WEEKLY_MODEL_TRAIN\""
				}
			},
			"name": "constantgenerator11"
		},
		"constantgenerator11": {
			"component": "com.sap.util.constantGenerator",
			"metadata": {
				"label": "Constant Generator",
				"x": 17,
				"y": 42,
				"height": 80,
				"width": 120,
				"extensible": true,
				"config": {
					"content": "SELECT  \"week_ending_date\", \"retailer\", \"state\", \"business\", \"category\", \"brand\", \"ppg\", \"week_of_year\", \"pos_qty_ty\", \"pos_dollar_ty\", \"FillMean\"  FROM     \"SEP_COVIDEXT\".\"Z_SEP.AnalyticalModels.SCM.DemandForecasting.CovidExternal::TA_SCM_COVID_FULL_SAMPLE_TRAIN\"   where \"state\" ='AK'",
					"counter": 0
				}
			},
			"name": "constantgenerator1"
		},
		"saphanaclient111": {
			"component": "com.sap.hana.client2",
			"metadata": {
				"label": "EXTERNAL_MERGE_WEEKLY",
				"x": 201.99999904632568,
				"y": 162,
				"height": 80,
				"width": 120,
				"config": {
					"connection": {
						"configurationType": "Configuration Manager",
						"connectionID": "EVHANADB"
					},
					"tableName": "\"SEP_COVIDEXT\".\"Z_SEP.AnalyticalModels.SCM.DemandForecasting.CovidExternal::TA_SCM_COVID_EXT_WEEKLY_MODEL_TRAIN\"",
					"csvHeader": "Ignore",
					"tableColumns": [
						{
							"name": "\"date\"",
							"type": "DATE"
						},
						{
							"name": "\"state\"",
							"type": "NVARCHAR",
							"size": 50
						},
						{
							"name": "\"state_initial\"",
							"type": "NVARCHAR",
							"size": 50
						},
						{
							"name": "\"AT_adj\"",
							"type": "DOUBLE"
						},
						{
							"name": "\"food_cpi_nat_mth\"",
							"type": "DOUBLE"
						},
						{
							"name": "\"snap_cost_st_mth\"",
							"type": "DOUBLE"
						},
						{
							"name": "\"allbed_mean\"",
							"type": "DOUBLE"
						},
						{
							"name": "\"confirmed_infections\"",
							"type": "DOUBLE"
						},
						{
							"name": "\"deaths_mean\"",
							"type": "DOUBLE"
						},
						{
							"name": "\"est_infections_mean\"",
							"type": "DOUBLE"
						},
						{
							"name": "\"mobility_composite_wors\"",
							"type": "DOUBLE"
						},
						{
							"name": "\"states_on_stay_home\"",
							"type": "INTEGER"
						},
						{
							"name": "\"states_on_travel_limit\"",
							"type": "INTEGER"
						},
						{
							"name": "\"states_on_any_business\"",
							"type": "INTEGER"
						},
						{
							"name": "\"states_on_all_non-ess_business\"",
							"type": "INTEGER"
						},
						{
							"name": "\"states_on_any_gathering_restrict\"",
							"type": "INTEGER"
						},
						{
							"name": "\"states_on_educational_fac\"",
							"type": "INTEGER"
						}
					]
				}
			},
			"name": "saphanaclient11"
		},
		"saphanaclient11": {
			"component": "com.sap.hana.client2",
			"metadata": {
				"label": "FULL_SAMPLE",
				"x": 201.99999904632568,
				"y": 42,
				"height": 80,
				"width": 120,
				"config": {
					"connection": {
						"configurationType": "Configuration Manager",
						"connectionID": "EVHANADB"
					},
					"tableName": "\"SEP_COVIDEXT\".\"Z_SEP.AnalyticalModels.SCM.DemandForecasting.CovidExternal::TA_SCM_COVID_FULL_SAMPLE_TRAIN\"",
					"csvHeader": "Ignore",
					"tableColumns": [
						{
							"name": "\"week_ending_date\"",
							"type": "DATE"
						},
						{
							"name": "\"retailer\"",
							"type": "NVARCHAR",
							"size": 100
						},
						{
							"name": "\"state\"",
							"type": "NVARCHAR",
							"size": 50
						},
						{
							"name": "\"business\"",
							"type": "NVARCHAR",
							"size": 100
						},
						{
							"name": "\"category\"",
							"type": "NVARCHAR",
							"size": 50
						},
						{
							"name": "\"brand\"",
							"type": "NVARCHAR",
							"size": 100
						},
						{
							"name": "\"ppg\"",
							"type": "NVARCHAR",
							"size": 100
						},
						{
							"name": "\"week_of_year\"",
							"type": "INTEGER"
						},
						{
							"name": "\"pos_qty_ty\"",
							"type": "DOUBLE"
						},
						{
							"name": "\"pos_dollar_ty\"",
							"type": "DOUBLE"
						},
						{
							"name": "\"FillMean\"",
							"type": "DOUBLE"
						}
					],
					"networkBatchSize": 5000,
					"connectionTimeoutInMs": 50000
				}
			},
			"name": "saphanaclient1"
		},
		"tostringconverter1": {
			"component": "com.sap.util.toStringConverter",
			"metadata": {
				"label": "ToString Converter",
				"x": 386.99999809265137,
				"y": 72,
				"height": 50,
				"width": 50,
				"config": {}
			}
		},
		"tostringconverter11": {
			"component": "com.sap.util.toStringConverter",
			"metadata": {
				"label": "ToString Converter",
				"x": 386.99999809265137,
				"y": 162,
				"height": 50,
				"width": 50,
				"config": {}
			},
			"name": "tostringconverter1"
		},
		"saphanaclient12": {
			"component": "com.sap.hana.client2",
			"metadata": {
				"label": "SAP HANA Client",
				"x": 854.9999942779541,
				"y": 192,
				"height": 80,
				"width": 120,
				"config": {
					"connection": {
						"configurationType": "Configuration Manager",
						"connectionID": "EVHANADB"
					},
					"tableName": "\"SEP_COVIDEXT\".\"TA_SCM_COVID_LONGTERM_FORECAST_OUTPUT_MULTIPLICITY\"",
					"tableColumns": [
						{
							"name": "\"retailer\"",
							"type": "NVARCHAR",
							"size": 100
						},
						{
							"name": "\"state\"",
							"type": "NVARCHAR",
							"size": 50
						},
						{
							"name": "\"brand\"",
							"type": "NVARCHAR",
							"size": 100
						},
						{
							"name": "\"ppg\"",
							"type": "NVARCHAR",
							"size": 100
						},
						{
							"name": "\"food_cpi_nat_mth\"",
							"type": "DOUBLE"
						},
						{
							"name": "\"snap_cost_st_mth\"",
							"type": "DOUBLE"
						},
						{
							"name": "\"week_ending_date\"",
							"type": "NVARCHAR",
							"size": 300
						},
						{
							"name": "\"FillMean\"",
							"type": "DOUBLE"
						}
					],
					"initTable": "Drop (Cascade)"
				}
			},
			"name": "saphanaclient1"
		},
		"terminal1": {
			"component": "com.sap.util.terminal",
			"metadata": {
				"label": "Terminal",
				"x": 1039.9999933242798,
				"y": 42,
				"height": 80,
				"width": 120,
				"ui": "dynpath",
				"config": {}
			}
		},
		"tostringconverter2": {
			"component": "com.sap.util.toStringConverter",
			"metadata": {
				"label": "ToString Converter",
				"x": 889.4999942779541,
				"y": 12,
				"height": 50,
				"width": 50,
				"config": {}
			}
		},
		"terminal2": {
			"component": "com.sap.util.terminal",
			"metadata": {
				"label": "Terminal",
				"x": 1039.9999933242798,
				"y": 162,
				"height": 80,
				"width": 120,
				"ui": "dynpath",
				"config": {}
			}
		},
		"tostringconverter3": {
			"component": "com.sap.util.toStringConverter",
			"metadata": {
				"label": "ToString Converter",
				"x": 889.4999942779541,
				"y": 102,
				"height": 50,
				"width": 50,
				"config": {}
			}
		}
	},
	"groups": [
		{
			"name": "group1",
			"nodes": [
				"python3operator1111"
			],
			"metadata": {
				"description": "Group"
			},
			"restartPolicy": "restart",
			"tags": {
				"CP": ""
			},
			"multiplicity": 5
		}
	],
	"connections": [
		{
			"metadata": {
				"points": "141,82 168.99999952316284,82 168.99999952316284,73 196.99999904632568,73"
			},
			"src": {
				"port": "out",
				"process": "constantgenerator11"
			},
			"tgt": {
				"port": "sql",
				"process": "saphanaclient11"
			}
		},
		{
			"metadata": {
				"points": "440.99999809265137,97 468.9999976158142,97 468.9999976158142,144.5 544.9999966621399,144.5 544.9999966621399,141 572.9999961853027,141"
			},
			"src": {
				"port": "outstring",
				"process": "tostringconverter1"
			},
			"tgt": {
				"port": "input2",
				"process": "python3operator1111"
			}
		},
		{
			"metadata": {
				"points": "141,202 168.99999952316284,202 168.99999952316284,193 196.99999904632568,193"
			},
			"src": {
				"port": "out",
				"process": "constantgenerator111"
			},
			"tgt": {
				"port": "sql",
				"process": "saphanaclient111"
			}
		},
		{
			"metadata": {
				"points": "440.99999809265137,187 468.9999976158142,187 468.9999976158142,155.5 544.9999966621399,155.5 544.9999966621399,159 572.9999961853027,159"
			},
			"src": {
				"port": "outstring",
				"process": "tostringconverter11"
			},
			"tgt": {
				"port": "input1",
				"process": "python3operator1111"
			}
		},
		{
			"metadata": {
				"points": "325.9999990463257,82 353.9999985694885,82 353.9999985694885,88 381.99999809265137,88"
			},
			"src": {
				"port": "result",
				"process": "saphanaclient11"
			},
			"tgt": {
				"port": "ininterface",
				"process": "tostringconverter1"
			}
		},
		{
			"metadata": {
				"points": "325.9999990463257,202 353.9999985694885,202 353.9999985694885,178 381.99999809265137,178"
			},
			"src": {
				"port": "result",
				"process": "saphanaclient111"
			},
			"tgt": {
				"port": "ininterface",
				"process": "tostringconverter11"
			}
		},
		{
			"metadata": {
				"points": "701.9999961853027,168 729.9999957084656,168 729.9999957084656,161 805.9999947547913,161 805.9999947547913,241 849.9999942779541,241"
			},
			"src": {
				"port": "output3",
				"process": "python3operator1111"
			},
			"tgt": {
				"port": "data",
				"process": "saphanaclient12"
			}
		},
		{
			"metadata": {
				"points": "701.9999961853027,132 729.9999957084656,132 729.9999957084656,139 805.9999947547913,139 805.9999947547913,46 884.4999942779541,46"
			},
			"src": {
				"port": "output2",
				"process": "python3operator1111"
			},
			"tgt": {
				"port": "inmessage",
				"process": "tostringconverter2"
			}
		},
		{
			"metadata": {
				"points": "943.4999942779541,37 1006.999993801117,37 1006.999993801117,82 1034.9999933242798,82"
			},
			"src": {
				"port": "outstring",
				"process": "tostringconverter2"
			},
			"tgt": {
				"port": "in1",
				"process": "terminal1"
			}
		},
		{
			"metadata": {
				"points": "701.9999961853027,150 821.9999947547913,150 821.9999947547913,136 884.4999942779541,136"
			},
			"src": {
				"port": "output1",
				"process": "python3operator1111"
			},
			"tgt": {
				"port": "inmessage",
				"process": "tostringconverter3"
			}
		},
		{
			"metadata": {
				"points": "943.4999942779541,127 1006.999993801117,127 1006.999993801117,202 1034.9999933242798,202"
			},
			"src": {
				"port": "outstring",
				"process": "tostringconverter3"
			},
			"tgt": {
				"port": "in1",
				"process": "terminal2"
			}
		}
	],
	"inports": {},
	"outports": {}
}